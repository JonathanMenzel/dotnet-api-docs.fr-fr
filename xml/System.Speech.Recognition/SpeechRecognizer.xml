<Type Name="SpeechRecognizer" FullName="System.Speech.Recognition.SpeechRecognizer">
  <Metadata>
    <Meta Name="ms.openlocfilehash" Value="965769d19b2d6dcdf59a744b2878c15d9d192ecc" />
    <Meta Name="ms.sourcegitcommit" Value="434f60616a9793fa8436744549fc856e94f7a648" />
    <Meta Name="ms.translationtype" Value="HT" />
    <Meta Name="ms.contentlocale" Value="fr-FR" />
    <Meta Name="ms.lasthandoff" Value="08/24/2018" />
    <Meta Name="ms.locfileid" Value="37610796" />
  </Metadata>
  <TypeSignature Language="C#" Value="public class SpeechRecognizer : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognizer extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognizer" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognizer&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognizer : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognizer = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Permet d'accéder au service de reconnaissance vocale partagé disponible sur le Bureau Windows.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Applications utilisent le module de reconnaissance partagé pour accéder à la reconnaissance vocale Windows. Utilisez le <xref:System.Speech.Recognition.SpeechRecognizer> objet à ajouter à l’expérience utilisateur de reconnaissance vocale Windows.  
  
 Cette classe permet de contrôler des différents aspects du processus de reconnaissance vocale :  
  
-   Pour gérer le syntaxe de reconnaissance vocale, utilisez le <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A>, et <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A>.  
  
-   Pour obtenir plus d’informations sur la reconnaissance vocale actuel opérations de reconnaissance, s’abonner à la <xref:System.Speech.Recognition.SpeechRecognizer>de <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> événements.  
  
-   Pour afficher ou modifier le nombre d’autres résultats retourne le module de reconnaissance, utilisez le <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> propriété. Le module de reconnaissance retourne des résultats de reconnaissance dans un <xref:System.Speech.Recognition.RecognitionResult> objet.  
  
-   Pour accéder ou surveiller l’état du module de reconnaissance partagée, utilisez le <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>, et <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> propriétés et le <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred>, <xref:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged>, et <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> événements.  
  
-   Pour synchroniser les modifications dans le module de reconnaissance, utilisez le <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> (méthode). Le module de reconnaissance partagé utilise plusieurs threads pour effectuer des tâches.  
  
-   Pour émuler l’entrée dans le module de reconnaissance partagé, utilisez le <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> et <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> méthodes.  
  
 La configuration de la reconnaissance vocale Windows est gérée par l’utilisation de la **propriétés Parole** boîte de dialogue dans le **le panneau de configuration**. Cette interface est utilisée pour sélectionner le moteur de reconnaissance vocale par défaut, la langue, le périphérique d’entrée audio et le comportement de mise en veille de la reconnaissance vocale. Si la configuration de la reconnaissance vocale Windows est modifiée pendant l’exécution de l’application, (par exemple, si la reconnaissance vocale est désactivée ou la langue d’entrée est modifiée), la modification affecte toutes les <xref:System.Speech.Recognition.SpeechRecognizer> objets.  
  
 Pour créer un module de reconnaissance vocale de dans le processus qui est indépendante de la reconnaissance vocale Windows, utilisez la <xref:System.Speech.Recognition.SpeechRecognitionEngine> classe.  
  
> [!NOTE]
>  Appelez toujours <xref:System.Speech.Recognition.SpeechRecognizer.Dispose%2A> avant de libérer votre dernière référence à la reconnaissance vocale. Sinon, les ressources qu’il utilise ne seront pas libérées tant que le garbage collector n’appelle l’objet de module de reconnaissance `Finalize` (méthode).  
  
   
  
## Examples  
 L’exemple suivant fait partie d’une application console qui charge une syntaxe de reconnaissance vocale et illustre l’entrée émulée asynchrone, les résultats de reconnaissance associées et les événements associés déclenchés par le module de reconnaissance vocale.  Si la reconnaissance vocale Windows n’est pas en cours d’exécution, avant de démarrer cette application sera également démarrer la reconnaissance vocale Windows. Si la reconnaissance vocale Windows est dans le **couchage** d’état, puis <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> retourne toujours la valeur null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.   
        // This matches the grammar and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.  
        // This does not match the grammar or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the SpeechRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
  </Docs>
  <Members>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognizer ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognizer();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Initialise une nouvelle instance de la classe <see cref="T:System.Speech.Recognition.SpeechRecognizer" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Chaque <xref:System.Speech.Recognition.SpeechRecognizer> objet conserve un ensemble distinct de la syntaxe de reconnaissance vocale.  
  
   
  
## Examples  
 L’exemple suivant fait partie d’une application console qui charge une syntaxe de reconnaissance vocale et illustre l’entrée émulée asynchrone, les résultats de reconnaissance associées et les événements associés déclenchés par le module de reconnaissance vocale. Si la reconnaissance vocale Windows n’est pas en cours d’exécution, avant de démarrer cette application sera également démarrer la reconnaissance vocale Windows. Si la reconnaissance vocale Windows est dans le **couchage** d’état, puis <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> retourne toujours la valeur null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.   
        // This matches the grammar and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // Start asynchronous emulated recognition.  
        // This does not match the grammar or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the SpeechRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient le format de l'audio reçu par le module de reconnaissance vocale.</summary>
        <value>Format d'entrée audio pour le module de reconnaissance vocale ou <see langword="null" /> si l'entrée fournie au module de reconnaissance n'est pas configurée.</value>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient le niveau de l'audio reçu par le module de reconnaissance vocale.</summary>
        <value>Niveau sonore de l'entrée fournie au module de reconnaissance vocale, compris entre 0 et 100.</value>
        <remarks>To be added.</remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se produit lorsque le module de reconnaissance partagé indique le niveau de son entrée audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance déclenche cet événement plusieurs fois par seconde. La fréquence avec laquelle l’événement est déclenché dépend de l’ordinateur sur lequel l’application est en cours d’exécution.  
  
 Pour obtenir le niveau audio au moment de l’événement, utilisez le <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> propriété associé <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Pour obtenir le niveau audio actuel de l’entrée dans le module de reconnaissance, utilisez le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> propriété.  
  
 Lorsque vous créez un délégué pour un `AudioLevelUpdated` événement, que vous identifiez la méthode qui gérera l’événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant ajoute un gestionnaire pour le `AudioLevelUpdated` événement à un <xref:System.Speech.Recognition.SpeechRecognizer> objet. Le gestionnaire génère le nouveau niveau audio à la console.  
  
```csharp  
private SpeechRecognizer recognizer;  
  
// Initialize the SpeechRecognizer object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognizer();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
    new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient la position actuelle dans le flux audio généré par le périphérique qui gère les entrées pour le module de reconnaissance vocale.</summary>
        <value>Position actuelle du flux d'entrée audio du module de reconnaissance vocale via lequel a été reçue l'entrée.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance partagé reçoit l’entrée pendant l’exécution de la reconnaissance vocale.  
  
 Le `AudioPosition` propriété référence la position du périphérique d’entrée dans son flux audio généré. En revanche, le <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> propriété référence la position du module de reconnaissance lors du traitement de l’entrée audio. Ces positions peuvent être différentes.  Par exemple, si le module de reconnaissance a reçu d’entrée pour lesquels il n’a pas encore généré un résultat de reconnaissance ensuite la valeur de la <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> propriété est inférieure à la valeur de la <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> propriété.  
  
   
  
## Examples  
 Dans l’exemple suivant, le module de reconnaissance vocale partagé utilise une syntaxe de dictée pour faire correspondre la saisie vocale. Un gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> événement écrit dans la console le <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A>, et <xref:System.Speech.Recognition.SpeechRecognizer.AudioLevel%2A> lorsque le module de reconnaissance vocale détecte une reconnaissance vocale à son entrée.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Add handlers for events.  
      recognizer.LoadGrammarCompleted +=   
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
      recognizer.SpeechRecognized +=   
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.StateChanged +=   
        new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
      recognizer.SpeechDetected +=   
        new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load the grammar object to the recognizer.  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Recognizer audio position: " + recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Write the name of the loaded grammar to the console.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se produit lorsque le module de reconnaissance rencontre un problème dans le signal audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Pour obtenir le problème s’est produit, utilisez le <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> propriété associé <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Lorsque vous créez un délégué pour un `AudioSignalProblemOccurred` événement, que vous identifiez la méthode qui gérera l’événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant définit un gestionnaire d’événements qui rassemble des informations sur un `AudioSignalProblemOccurred` événement.  
  
```  
private SpeechRecognizer recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognizer();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognizer.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient l'état de l'audio reçu par le module de reconnaissance vocale.</summary>
        <value>État de l'entrée audio dans le module de reconnaissance vocale.</value>
        <remarks>To be added.</remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se produit lors du changement de l'état de l'audio reçu par le module de reconnaissance.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Pour obtenir l’état audio au moment de l’événement, utilisez le <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> propriété associé <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Pour obtenir l’état audio actuel de l’entrée dans le module de reconnaissance, utilisez le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> propriété. Pour plus d’informations sur l’état audio, consultez le <xref:System.Speech.Recognition.AudioState> énumération.  
  
 Lorsque vous créez un délégué pour un `AudioStateChanged` événement, que vous identifiez la méthode qui gérera l’événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant utilise un gestionnaire pour le `AudioStateChanged` événement à écrire le module de reconnaissance du nouveau <xref:System.Speech.Recognition.SpeechRecognizer.AudioState%2A> à la console chaque fois que cela change à l’aide d’un membre de la <xref:System.Speech.Recognition.AudioState> énumération.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.StateChanged +=  
          new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Put the recognizer into Listening mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        Console.WriteLine();  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Supprime l'objet <see cref="T:System.Speech.Recognition.SpeechRecognizer" />.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognizer.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Supprime l'objet <see cref="T:System.Speech.Recognition.SpeechRecognizer" />.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognizer.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">
          <see langword="true" /> pour libérer les ressources managées et non managées ; <see langword="false" /> pour ne libérer que les ressources non managées.</param>
        <summary>Supprime l’objet <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> et libère les ressources utilisées pendant la session.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Émule l'entrée dans le module de reconnaissance vocale partagé, en utilisant le texte au lieu de l'audio pour la reconnaissance vocale synchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ces méthodes contournent l’entrée audio du système. Cela peut être utile lorsque vous testez ou le débogage d’une application ou grammaire.  
  
> [!NOTE]
>  Si la reconnaissance vocale Windows est dans le **couchage** d’état, puis ces méthodes retournent `null`.  
  
 Le module de reconnaissance partagé déclenche le <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> événements comme si l’opération de reconnaissance n’est pas émulée. Le module de reconnaissance ignore les nouvelles lignes et l’espace blanc superflu et traite des signes de ponctuation en tant qu’entrée littéral.  
  
> [!NOTE]
>  Le <xref:System.Speech.Recognition.RecognitionResult> objet généré par le module de reconnaissance partagé en réponse à l’entrée émulée a la valeur `null` pour son <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> propriété.  
  
 Pour émuler le module de reconnaissance asynchrone, utilisez la <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> (méthode).  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognizer.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Entrée de l'opération de reconnaissance.</param>
        <summary>Émule l'entrée d'une expression dans le module de reconnaissance vocale partagé, en utilisant le texte au lieu de l'audio pour la reconnaissance vocale synchrone.</summary>
        <returns>Résultat de la reconnaissance pour l’opération de reconnaissance ou <see langword="null" />, si l’opération n’est pas réussie ou si la reconnaissance vocale Windows est dans l’état de **veille**.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Les modules de reconnaissance fournis avec Vista et Windows 7 ignorent la casse et la largeur de caractères lors de l’application des règles de grammaire à l’expression d’entrée. Pour plus d’informations sur ce type de comparaison, consultez le <xref:System.Globalization.CompareOptions> valeurs d’énumération <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> et <xref:System.Globalization.CompareOptions.IgnoreWidth>. Les modules de reconnaissance également ignorent les nouvelles lignes et les espaces superflus et traitent des signes de ponctuation en tant qu’entrée littéral.  
  
   
  
## Examples  
 L’exemple suivant charge une syntaxe d’exemple dans le module de reconnaissance partagé et émule l’entrée dans le module de reconnaissance. Si la reconnaissance vocale Windows n’est pas en cours d’exécution, avant de démarrer cette application sera également démarrer la reconnaissance vocale Windows. Si la reconnaissance vocale Windows est dans le **couchage** d’état, puis <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> retourne toujours la valeur null.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        RecognitionResult result;  
  
        // This EmulateRecognize call matches the grammar and returns a  
        // recognition result.  
        result = recognizer.EmulateRecognize("testing testing");  
        OutputResult(result);  
  
        // This EmulateRecognize call does not match the grammar and   
        // returns null.  
        result = recognizer.EmulateRecognize("testing one two three");  
        OutputResult(result);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Output information about a recognition result to the console.  
    private static void OutputResult(RecognitionResult result)  
    {  
      if (result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognizer.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Un tableau d'unités de mot qui contient l'entrée pour l'opération de reconnaissance.</param>
        <param name="compareOptions">Combinaison d'opérations de bits des valeurs d'énumération qui décrivent le type de comparaison à utiliser pour la reconnaissance émulée.</param>
        <summary>Émule l'entrée de mots spécifiques dans le module de reconnaissance vocale partagé, en utilisant le texte au lieu de l'audio pour la reconnaissance vocale synchrone, et spécifie comment le module de reconnaissance gère la comparaison Unicode entre les mots et les grammaires de la reconnaissance vocale chargées.</summary>
        <returns>Résultat de la reconnaissance pour l’opération de reconnaissance ou <see langword="null" />, si l’opération n’est pas réussie ou si la reconnaissance vocale Windows est dans l’état de **veille**.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cette méthode crée un <xref:System.Speech.Recognition.RecognitionResult> de l’objet en utilisant les informations fournies dans le `wordUnits` paramètre.  
  
 Le module de reconnaissance utilise le `compareOptions` quand elle s’applique les règles de grammaire à l’expression d’entrée. Les modules de reconnaissance fournis avec Vista et Windows 7 ignorent la casse si le <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> ou <xref:System.Globalization.CompareOptions.IgnoreCase> valeur est présente. Les modules de reconnaissance toujours ignorent la largeur des caractères et jamais ignorent le type Kana. Les modules de reconnaissance également ignorer les nouvelles lignes et les espaces superflus et traite des signes de ponctuation en tant qu’entrée littéral. Pour plus d’informations sur la largeur des caractères et le type Kana, consultez le <xref:System.Globalization.CompareOptions> énumération.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognizer.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Expression d'entrée de l'opération de reconnaissance.</param>
        <param name="compareOptions">Combinaison d'opérations de bits des valeurs d'énumération qui décrivent le type de comparaison à utiliser pour la reconnaissance émulée.</param>
        <summary>Émule l'entrée d'une expression dans le module de reconnaissance vocale partagé, en utilisant le texte au lieu de l'audio pour la reconnaissance vocale synchrone, et spécifie comment le module de reconnaissance gère la comparaison Unicode entre l'expression et les grammaires de la reconnaissance vocale chargées.</summary>
        <returns>Résultat de la reconnaissance pour l’opération de reconnaissance ou <see langword="null" />, si l’opération n’est pas réussie ou si la reconnaissance vocale Windows est dans l’état de **veille**.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance utilise le `compareOptions` quand elle s’applique les règles de grammaire à l’expression d’entrée. Les modules de reconnaissance fournis avec Vista et Windows 7 ignorent la casse si le <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> ou <xref:System.Globalization.CompareOptions.IgnoreCase> valeur est présente. Les modules de reconnaissance toujours ignorent la largeur des caractères et jamais ignorent le type Kana. Les modules de reconnaissance également ignorer les nouvelles lignes et les espaces superflus et traite des signes de ponctuation en tant qu’entrée littéral. Pour plus d’informations sur la largeur des caractères et le type Kana, consultez le <xref:System.Globalization.CompareOptions> énumération.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Émule l'entrée dans le module de reconnaissance vocale partagé, en utilisant le texte au lieu de l'audio pour la reconnaissance vocale asynchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ces méthodes contournent l’entrée audio du système. Cela peut être utile lorsque vous testez ou le débogage d’une application ou grammaire.  
  
 Le module de reconnaissance partagé déclenche le <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> événements comme si l’opération de reconnaissance n’est pas émulée. Lorsque le module de reconnaissance termine l’opération de reconnaissance asynchrone, il déclenche le <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> événement. Le module de reconnaissance ignore les nouvelles lignes et l’espace blanc superflu et traite des signes de ponctuation en tant qu’entrée littéral.  
  
> [!NOTE]
>  Si la reconnaissance vocale Windows est dans le **couchage** d’état, puis le module de reconnaissance partagé ne traite pas d’entrée et ne génère pas le <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> et événements connexes, mais toujours déclenche le <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> événement.  
  
> [!NOTE]
>  Le <xref:System.Speech.Recognition.RecognitionResult> objet généré par le module de reconnaissance partagé en réponse à l’entrée émulée a la valeur `null` pour son <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> propriété.  
  
 Pour émuler la reconnaissance synchrone, utilisez la <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> (méthode).  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognizer.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Entrée de l'opération de reconnaissance.</param>
        <summary>Émule l'entrée d'une expression dans le module de reconnaissance vocale partagé, en utilisant le texte au lieu de l'audio pour la reconnaissance vocale asynchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Les modules de reconnaissance fournis avec Vista et Windows 7 ignorent la casse et la largeur de caractères lors de l’application des règles de grammaire à l’expression d’entrée. Pour plus d’informations sur ce type de comparaison, consultez le <xref:System.Globalization.CompareOptions> valeurs d’énumération <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> et <xref:System.Globalization.CompareOptions.IgnoreWidth>. Les modules de reconnaissance également ignorent les nouvelles lignes et les espaces superflus et traitent des signes de ponctuation en tant qu’entrée littéral.  
  
   
  
## Examples  
 L’exemple suivant fait partie d’une application console qui charge une syntaxe de reconnaissance vocale et illustre l’entrée émulée asynchrone, les résultats de reconnaissance associées et les événements associés déclenchés par le module de reconnaissance vocale. Si la reconnaissance vocale Windows n’est pas en cours d’exécution, avant de démarrer cette application sera également démarrer la reconnaissance vocale Windows. Si la reconnaissance vocale Windows est dans le **couchage** d’état, puis <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> retourne toujours la valeur null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar   
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.   
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognizer.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Un tableau d'unités de mot qui contient l'entrée pour l'opération de reconnaissance.</param>
        <param name="compareOptions">Combinaison d'opérations de bits des valeurs d'énumération qui décrivent le type de comparaison à utiliser pour la reconnaissance émulée.</param>
        <summary>Émule l'entrée de mots spécifiques dans le module de reconnaissance vocale partagé, en utilisant le texte au lieu de l'audio pour la reconnaissance vocale asynchrone, et spécifie comment le module de reconnaissance gère la comparaison Unicode entre les mots et les grammaires de la reconnaissance vocale chargées.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cette méthode crée un <xref:System.Speech.Recognition.RecognitionResult> de l’objet en utilisant les informations fournies dans le `wordUnits` paramètre.  
  
 Le module de reconnaissance utilise le `compareOptions` quand elle s’applique les règles de grammaire à l’expression d’entrée. Les modules de reconnaissance fournis avec Vista et Windows 7 ignorent la casse si le <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> ou <xref:System.Globalization.CompareOptions.IgnoreCase> valeur est présente. Les modules de reconnaissance toujours ignorent la largeur des caractères et jamais ignorent le type Kana. Les modules de reconnaissance également ignorer les nouvelles lignes et les espaces superflus et traite des signes de ponctuation en tant qu’entrée littéral. Pour plus d’informations sur la largeur des caractères et le type Kana, consultez le <xref:System.Globalization.CompareOptions> énumération.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognizer.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Expression d'entrée de l'opération de reconnaissance.</param>
        <param name="compareOptions">Combinaison d'opérations de bits des valeurs d'énumération qui décrivent le type de comparaison à utiliser pour la reconnaissance émulée.</param>
        <summary>Émule l'entrée d'une expression dans le module de reconnaissance vocale partagé, en utilisant le texte au lieu de l'audio pour la reconnaissance vocale asynchrone, et spécifie comment le module de reconnaissance gère la comparaison Unicode entre l'expression et les grammaires de la reconnaissance vocale chargées.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance utilise le `compareOptions` quand elle s’applique les règles de grammaire à l’expression d’entrée. Les modules de reconnaissance fournis avec Vista et Windows 7 ignorent la casse si le <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> ou <xref:System.Globalization.CompareOptions.IgnoreCase> valeur est présente. Les modules de reconnaissance toujours ignorent la largeur des caractères et jamais ignorent le type Kana. Les modules de reconnaissance également ignorer les nouvelles lignes et les espaces superflus et traite des signes de ponctuation en tant qu’entrée littéral. Pour plus d’informations sur la largeur des caractères et le type Kana, consultez le <xref:System.Globalization.CompareOptions> énumération.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se produit lorsque le module de reconnaissance partagé finalise un module de reconnaissance asynchrone pour l'entrée émulée.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Chaque <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> méthode commence une opération de reconnaissance asynchrone. Le module de reconnaissance déclenche le `EmulateRecognizeCompleted` événement lorsqu’il finalise l’opération asynchrone.  
  
 L’opération de reconnaissance asynchrone peut déclencher la <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> événements. Le <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted> événement est le dernier événement de ce type que le module de reconnaissance se déclenche pour une opération donnée.  
  
 Lorsque vous créez un délégué pour un `EmulateRecognizeCompleted` événement, que vous identifiez la méthode qui gérera l’événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant fait partie d’une application console qui charge une syntaxe de reconnaissance vocale et illustre l’entrée émulée asynchrone, les résultats de reconnaissance associées et les événements associés déclenchés par le module de reconnaissance vocale. Si la reconnaissance vocale Windows n’est pas en cours d’exécution, avant de démarrer cette application sera également démarrer la reconnaissance vocale Windows. Si la reconnaissance vocale Windows est dans le **couchage** mode, puis <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> retourne toujours la valeur null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=   
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="Enabled">
      <MemberSignature Language="C#" Value="public bool Enabled { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance bool Enabled" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
      <MemberSignature Language="VB.NET" Value="Public Property Enabled As Boolean" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property bool Enabled { bool get(); void set(bool value); };" />
      <MemberSignature Language="F#" Value="member this.Enabled : bool with get, set" Usage="System.Speech.Recognition.SpeechRecognizer.Enabled" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Boolean</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient ou définit une valeur qui indique si cet objet de <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> est prêt à procéder à la reconnaissance vocale.</summary>
        <value>
          <see langword="true" /> si cet objet <see cref="T:System.Speech.Recognition.SpeechRecognizer" /> exécute la reconnaissance vocale ; sinon, <see langword="false" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Modifications apportées à cette propriété n’affectent pas les autres instances de la <xref:System.Speech.Recognition.SpeechRecognizer> classe.  
  
 Par défaut, la valeur de la <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> propriété est `true` pour une instance nouvellement instanciée de <xref:System.Speech.Recognition.SpeechRecognizer>. Alors que le module de reconnaissance est désactivé, aucun des grammaires de la reconnaissance du module de reconnaissance vocale sont disponibles pour les opérations de reconnaissance. Définition du module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> propriété n’a aucun effet sur le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> propriété.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognizer.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient une collection des objets <see cref="T:System.Speech.Recognition.Grammar" /> chargés dans cette instance <see cref="T:System.Speech.Recognition.SpeechRecognizer" />.</summary>
        <value>Collection d'objets <see cref="T:System.Speech.Recognition.Grammar" /> que l'application a chargé dans l'instance actuelle du module de reconnaissance partagé.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cette propriété ne retourne pas de n’importe quel discours syntaxe de reconnaissance chargée par une autre application.  
  
   
  
## Examples  
 L’exemple suivant génère des informations sur la console pour chaque syntaxe de reconnaissance vocale chargée dans le module de reconnaissance vocale partagé.  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        Grammar sampleGrammar = new Grammar(new GrammarBuilder("sample phrase"));  
        sampleGrammar.Name = "Sample Grammar";  
        recognizer.LoadGrammar(sampleGrammar);  
  
        OutputGrammarList(recognizer);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void OutputGrammarList(SpeechRecognizer recognizer)  
    {  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      if (grammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in grammars)  
        {  
          Console.WriteLine("  Grammar: {0}",  
            (g.Name != null) ? g.Name : "<no name>");  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
    }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognizer.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Syntaxe de reconnaissance vocale à charger.</param>
        <summary>Charge une grammaire de reconnaissance vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance partagé lève une exception si la grammaire de reconnaissance vocale est déjà chargée, est en cours de chargement en mode asynchrone ou n’a pas pu charger dans n’importe quel module de reconnaissance. Si le module de reconnaissance est en cours d’exécution, les applications doivent utiliser <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> pour suspendre le moteur de reconnaissance vocale avant le chargement, déchargement, activer ou désactiver une grammaire.  
  
 Pour charger une grammaire de reconnaissance vocale de façon asynchrone, utilisez la <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> (méthode).  
  
   
  
## Examples  
 L’exemple suivant fait partie d’une application console qui charge une syntaxe de reconnaissance vocale et illustre l’entrée émulée asynchrone, les résultats de reconnaissance associées et les événements associés déclenchés par le module de reconnaissance vocale. Si la reconnaissance vocale Windows n’est pas en cours d’exécution, avant de démarrer cette application sera également démarrer la reconnaissance vocale Windows. Si la reconnaissance vocale Windows est dans le **couchage** d’état, puis <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> retourne toujours la valeur null.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Initialize an instance of the shared recognizer.  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar   
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Recognition result = {0}",  
          e.Result.Text ?? "<no text>");  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }   
  
    // Handle the EmulateRecognizeCompleted event.   
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("No result generated.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognizer.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Syntaxe de reconnaissance vocale à charger.</param>
        <summary>Charge une syntaxe de reconnaissance vocale de façon asynchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Lorsque le module de reconnaissance termine à cette opération asynchrone, il déclenche une <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> événement. Le module de reconnaissance lève une exception si la grammaire de reconnaissance vocale est déjà chargée, est en cours de chargement en mode asynchrone ou n’a pas pu charger dans n’importe quel module de reconnaissance. Si le module de reconnaissance est en cours d’exécution, les applications doivent utiliser <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> pour suspendre le moteur de reconnaissance vocale avant le chargement, déchargement, activer ou désactiver une grammaire.  
  
 Pour charger une grammaire de reconnaissance vocale de façon synchrone, utilisez la <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammar%2A> (méthode).  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se produit lorsque le module de reconnaissance termine le chargement asynchrone d'une syntaxe de reconnaissance vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync%2A> méthode lance une opération asynchrone. Le module de reconnaissance déclenche le `LoadGrammarCompleted` événement lorsqu’il termine l’opération. Pour obtenir le <xref:System.Speech.Recognition.Grammar> que le module de reconnaissance est chargé de l’objet, utilisez le <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> propriété associé <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Pour obtenir des cours <xref:System.Speech.Recognition.Grammar> objets le module de reconnaissance a chargé, utilisez le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognizer.Grammars%2A> propriété.  
  
 Lorsque vous créez un délégué pour un `LoadGrammarCompleted` événement, que vous identifiez la méthode qui gérera l’événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant crée un module de reconnaissance vocale partagé et crée ensuite deux types de grammaires pour reconnaître des mots spécifiques et pour accepter de dictée gratuite. L’exemple de charge de façon asynchrone toutes les grammaires créés dans le module de reconnaissance. Gestionnaires pour le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognizer.LoadGrammarCompleted> et <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> écrire des événements dans la console le nom de la grammaire qui a été utilisé pour effectuer la reconnaissance et le texte de résultat de reconnaissance, respectivement.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Add a handler for the StateChanged event.  
        recognizer.StateChanged +=  
          new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
        // Create "yesno" grammar.  
        Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah}" });  
        SemanticResultValue yesValue =  
            new SemanticResultValue(yesChoices, (bool)true);  
        Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
        SemanticResultValue noValue =  
            new SemanticResultValue(noChoices, (bool)false);  
        SemanticResultKey yesNoKey =  
            new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
        Grammar yesnoGrammar = new Grammar(yesNoKey);  
        yesnoGrammar.Name = "yesNo";  
  
        // Create "done" grammar.  
        Grammar doneGrammar =  
          new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
        doneGrammar.Name = "Done";  
  
        // Create dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation";  
  
        // Load grammars to the recognizer.  
        recognizer.LoadGrammarAsync(yesnoGrammar);  
        recognizer.LoadGrammarAsync(doneGrammar);  
        recognizer.LoadGrammarAsync(dictation);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Put the shared speech recognizer into "listening" mode.   
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognizer.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient ou définit le nombre maximal de résultats de reconnaissance retourné pour chaque opération de reconnaissance par le module de reconnaissance partagé.</summary>
        <value>Nombre maximum d'autres résultats que le module de reconnaissance vocale retourne pour chaque opération de reconnaissance.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> propriété de la <xref:System.Speech.Recognition.RecognitionResult> classe contient la collection de <xref:System.Speech.Recognition.RecognizedPhrase> objets qui représentent des autres interprétations de candidat de l’entrée.  
  
 La valeur par défaut <xref:System.Speech.Recognition.SpeechRecognizer.MaxAlternates%2A> est 10.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.RecognitionResult.Alternates" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="PauseRecognizerOnRecognition">
      <MemberSignature Language="C#" Value="public bool PauseRecognizerOnRecognition { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance bool PauseRecognizerOnRecognition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      <MemberSignature Language="VB.NET" Value="Public Property PauseRecognizerOnRecognition As Boolean" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property bool PauseRecognizerOnRecognition { bool get(); void set(bool value); };" />
      <MemberSignature Language="F#" Value="member this.PauseRecognizerOnRecognition : bool with get, set" Usage="System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Boolean</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient ou définit une valeur qui indique si le module de reconnaissance partagé suspend des opérations de reconnaissance pendant qu'une application gère un événement de <see cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />.</summary>
        <value>
          <see langword="true" /> si le module de reconnaissance partagé attend pour traiter l'entrée alors qu'une application gère l'événement <see cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" /> ; sinon, <see langword="false" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Définissez cette propriété sur `true`, si dans le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Gestionnaire d’événements votre application doit modifier l’état du service de reconnaissance vocale ou modifier les grammaires de reconnaissance vocale chargée ou est activée avant que le service de reconnaissance vocale processus d’entrée.  
  
> [!NOTE]
>  Définition de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> propriété `true` , chacune <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Gestionnaire d’événements dans chaque application pour bloquer le service de reconnaissance vocale Windows.  
  
 Pour synchroniser les modifications apportées au module de reconnaissance partagé avec l’état de votre application, utilisez le <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> (méthode).  
  
 Lorsque <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> est `true`, pendant l’exécution de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Gestionnaire du service de reconnaissance vocale s’arrête et met en mémoire tampon nouvelle entrée audio qu’elles arrivent. Une fois le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Gestionnaire d’événements s’arrête, la reconnaissance de reprend de service de reconnaissance vocale et démarre le traitement des informations à partir de son tampon d’entrée.  
  
 Pour activer ou désactiver le service de reconnaissance vocale, utilisez le <xref:System.Speech.Recognition.SpeechRecognizer.Enabled%2A> propriété.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient la position actuelle du module de reconnaissance dans l'entrée audio qu'il gère.</summary>
        <value>Position du module de reconnaissance dans l'entrée audio qu'il gère.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le `RecognizerAudioPosition` propriété référence la position du module de reconnaissance lors du traitement de son entrée audio. En revanche, le <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> propriété référence la position du périphérique d’entrée dans son flux audio généré. Ces positions peuvent être différentes. Par exemple, si le module de reconnaissance a reçu d’entrée pour lesquels il n’a pas encore généré un résultat de reconnaissance ensuite la valeur de la <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> propriété est inférieure à la valeur de la <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> propriété.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognizer.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient les informations à propos du module de reconnaissance vocale partagé.</summary>
        <value>Informations sur le module de reconnaissance vocale partagé.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cette propriété retourne des informations sur le module de reconnaissance vocale en cours d’utilisation par la reconnaissance vocale Windows.  
  
   
  
## Examples  
 L’exemple suivant envoie des informations sur le module de reconnaissance partagé dans la console.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SharedRecognizer  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
        Console.WriteLine("Recognizer information for the shared recognizer:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.State" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se produit lorsque le module de reconnaissance s'interrompt pour synchroniser la reconnaissance et d'autres opérations.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Les applications doivent utiliser <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> pour suspendre une instance en cours d’exécution de <xref:System.Speech.Recognition.SpeechRecognizer> avant de modifier son <xref:System.Speech.Recognition.Grammar> objets. Par exemple, bien que le <xref:System.Speech.Recognition.SpeechRecognizer> est suspendu, vous pouvez charger, décharger, activer et désactiver <xref:System.Speech.Recognition.Grammar> objets. Le <xref:System.Speech.Recognition.SpeechRecognizer> déclenche cet événement lorsqu’il est prêt à accepter les modifications.  
  
 Lorsque vous créez un délégué pour un <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> événement, que vous identifiez la méthode qui gérera l’événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant montre une application console qui charge et décharge <xref:System.Speech.Recognition.Grammar> objets. L’application utilise le <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> méthode pour demander le moteur de reconnaissance vocale en pause afin qu’elle peut recevoir une mise à jour. L’application, puis charge ou décharge un <xref:System.Speech.Recognition.Grammar> objet.  
  
 À chaque mise à jour, un gestionnaire pour <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> d’événement écrit le nom et l’état d’actuellement chargés <xref:System.Speech.Recognition.Grammar> objets dans la console. Comme les grammaires sont chargées et déchargées, l’application reconnaît tout d’abord les noms des animaux de batterie de serveurs, puis les noms des animaux de batterie de serveurs et les noms des fruits, puis uniquement les noms de fruits.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Create the first grammar - Farm.  
      Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
      GrammarBuilder farm = new GrammarBuilder(animals);  
      Grammar farmAnimals = new Grammar(farm);  
      farmAnimals.Name = "Farm";  
  
      // Create the second grammar - Fruit.  
      Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
      GrammarBuilder favorite = new GrammarBuilder(fruit);  
      Grammar favoriteFruit = new Grammar(favorite);  
      favoriteFruit.Name = "Fruit";  
  
      // Attach event handlers.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.RecognizerUpdateReached +=  
        new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
      recognizer.StateChanged +=   
        new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
      // Load the Farm grammar.  
      recognizer.LoadGrammar(farmAnimals);  
      Console.WriteLine("Grammar Farm is loaded");  
  
      // Pause to recognize farm animals.  
      Thread.Sleep(7000);  
      Console.WriteLine();  
  
      // Request an update and load the Fruit grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.LoadGrammarAsync(favoriteFruit);  
      Thread.Sleep(5000);  
  
      // Request an update and unload the Farm grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.UnloadGrammar(farmAnimals);  
      Thread.Sleep(5000);  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
      if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  Grammar {0} is loaded and is {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Demande que le module de reconnaissance partagé soit suspendu et mette à jour son état.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Utilisez cette méthode pour synchroniser les modifications apportées au module de reconnaissance partagé. Par exemple, si vous chargez ou déchargez une grammaire de reconnaissance vocale pendant que le module de reconnaissance traite l’entrée, utilisez cette méthode et la <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> événement pour synchroniser le comportement de votre application avec l’état du module de reconnaissance.  
  
 Lorsque cette méthode est appelée, le module de reconnaissance suspend ou termine les opérations asynchrones et génère un <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> événement. Un <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> Gestionnaire d’événements pouvez ensuite modifier l’état du module de reconnaissance entre les opérations de reconnaissance.  
  
 Lorsque cette méthode est appelée :  
  
-   Si le module de reconnaissance ne traite pas d’entrée, le module de reconnaissance génère immédiatement la <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> événement.  
  
-   Si le module de reconnaissance est le traitement d’entrée qui se compose de silence ou de bruit de fond, le module de reconnaissance s’interrompt l’opération de reconnaissance et génère le <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> événement.  
  
-   Si le module de reconnaissance est le traitement d’entrée qui n’est pas constitué de silence ou de bruit de fond, le module de reconnaissance termine l’opération de reconnaissance, puis génère le <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> événement.  
  
 Bien que le module de reconnaissance gère la <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> événement :  
  
-   Le module de reconnaissance ne traite pas d’entrée et la valeur de la <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> propriété reste le même.  
  
-   Le module de reconnaissance continue à collecter d’entrée et la valeur de la <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> propriété peut être modifiée.  
  
 Déterminer si le module de reconnaissance partagé suspend des opérations de reconnaissance pendant une application gère un <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> événement, utilisez le <xref:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition%2A> propriété.  
  
   
  
## Examples  
 L’exemple suivant montre une application console qui charge et décharge <xref:System.Speech.Recognition.Grammar> objets. L’application utilise le <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> méthode pour demander le moteur de reconnaissance vocale en pause afin qu’elle peut recevoir une mise à jour. L’application, puis charge ou décharge un <xref:System.Speech.Recognition.Grammar> objet.  
  
 À chaque mise à jour, un gestionnaire pour <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> d’événement écrit le nom et l’état d’actuellement chargés <xref:System.Speech.Recognition.Grammar> objets dans la console. Comme les grammaires sont chargées et déchargées, l’application reconnaît tout d’abord les noms des animaux de batterie de serveurs, puis les noms des animaux de batterie de serveurs et les noms des fruits, puis uniquement les noms de fruits.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      recognizer = new SpeechRecognizer();  
  
      // Create the first grammar - Farm.  
      Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
      GrammarBuilder farm = new GrammarBuilder(animals);  
      Grammar farmAnimals = new Grammar(farm);  
      farmAnimals.Name = "Farm";  
  
      // Create the second grammar - Fruit.  
      Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
      GrammarBuilder favorite = new GrammarBuilder(fruit);  
      Grammar favoriteFruit = new Grammar(favorite);  
      favoriteFruit.Name = "Fruit";  
  
      // Attach event handlers.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
      recognizer.RecognizerUpdateReached +=  
        new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
  
      // Check to see if recognizer is loaded, wait if it is not loaded.  
      if (recognizer.State != RecognizerState.Listening)  
      {  
        Thread.Sleep(5000);  
  
        // Put recognizer in listening state.  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
  
      // Load the Farm grammar.  
      recognizer.LoadGrammar(farmAnimals);  
      Console.WriteLine("Grammar Farm is loaded");  
  
      // Pause to recognize farm animals.  
      Thread.Sleep(7000);  
      Console.WriteLine();  
  
      // Request an update and load the Fruit grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.LoadGrammarAsync(favoriteFruit);  
      Thread.Sleep(5000);  
  
      // Request an update and unload the Farm grammar.  
      recognizer.RequestRecognizerUpdate();  
      recognizer.UnloadGrammar(farmAnimals);  
      Thread.Sleep(5000);  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    public static void recognizer_RecognizerUpdateReached(object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      // At the update, get the names and enabled status of the currently loaded grammars.  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  Grammar {0} is loaded and is {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.PauseRecognizerOnRecognition" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognizer.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Demande que le module de reconnaissance partagé soit suspendu et mette à jour son état.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Lorsque le module de reconnaissance génère le <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> événement, le <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> propriété de la <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> est `null`.  
  
 Pour fournir un jeton d’utilisateur, utilisez la <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> ou <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> (méthode). Pour spécifier un décalage de position audio, utilisez le <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> (méthode).  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognizer.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Informations définies par l'utilisateur qui comporte des informations sur l'opération.</param>
        <summary>Demande que le module de reconnaissance partagé soit suspendu, mette à jour son état et fournisse un jeton utilisateur pour l'événement associé.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Lorsque le module de reconnaissance génère le <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> événement, le <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> propriété de la <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contient la valeur de la `userToken` paramètre.  
  
 Pour spécifier un décalage de position audio, utilisez le <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> (méthode).  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognizer.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Informations définies par l'utilisateur qui comporte des informations sur l'opération.</param>
        <param name="audioPositionAheadToRaiseUpdate">Offset de la <see cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" /> actuelle pour différer la demande.</param>
        <summary>Demande que le module de reconnaissance partagé soit suspendu, mette à jour son état et fournisse un offset et un jeton utilisateur pour l'événement associé.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance n’initie pas la demande de mise à jour du module de reconnaissance jusqu'à ce que le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> est égal à actuel <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> plus la valeur de la `audioPositionAheadToRaiseUpdate` paramètre.  
  
 Lorsque le module de reconnaissance génère le <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached> événement, le <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> propriété de la <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contient la valeur de la `userToken` paramètre.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se produit lorsque le module de reconnaissance détecte l'entrée qu'il peut identifier comme vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance partagé peut déclencher cet événement en réponse à l’entrée. Le <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> propriété associé <xref:System.Speech.Recognition.SpeechDetectedEventArgs> objet indique l’emplacement dans le flux d’entrée où le module de reconnaissance détectée vocale. Pour plus d’informations, consultez le <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> et <xref:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition%2A> propriétés et le <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A> et <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> méthodes.  
  
 Lorsque vous créez un délégué pour un <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> événement, que vous identifiez la méthode qui gérera l’événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant fait partie d’une application de console pour le choix des villes d’origine et de destination pour un vol. L’application reconnaît des expressions telles que « Je veux passage vers Miami à Chicago ».  L’exemple utilise le <xref:System.Speech.Recognition.SpeechRecognizer.SpeechDetected> événement en rapport le <xref:System.Speech.Recognition.SpeechRecognizer.AudioPosition%2A> chaque heure de voix est détectée.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=   
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.RecognizerAudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se produit lorsque le module de reconnaissance a identifié un mot ou des mots qui peuvent être un composant de plusieurs expressions complètes dans une syntaxe.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance partagé peut déclencher cet événement lorsque l’entrée est AMBIGUE. Par exemple, pour une grammaire de reconnaissance vocale qui prend en charge la reconnaissance de le » nouvelle de jeux, » ou « nouvelle partie, » » nouvelle, de jeux » est une entrée non équivoque, et « nouvelle game » est une entrée ambiguë.  
  
 Lorsque vous créez un délégué pour un <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> événement, que vous identifiez la méthode qui gérera l’événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant reconnaît des expressions telles que « Afficher la liste des artistes dans la catégorie jazz ». L’exemple utilise le <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized> événement pour afficher des fragments de phrase incomplètes dans la console qu’elles sont reconnues.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=   
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechHypothesizedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se produit lorsque le module de reconnaissance accepte les entrées qui ne correspondent pas à l'une des syntaxes de reconnaissance vocale qu'il a chargé.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance partagé déclenche cet événement s’il détermine qu’entrée ne correspond pas à une confiance suffisante des grammaires de reconnaissance vocale chargées. Le <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propriété de la <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contient le rejet <xref:System.Speech.Recognition.RecognitionResult> objet.  
  
 Seuils de niveau de confiance pour le module de reconnaissance partagé, géré par <xref:System.Speech.Recognition.SpeechRecognizer>, sont associés à un profil utilisateur et stocké dans le Registre Windows. Les applications ne doivent pas écrire de modifications dans le Registre pour les propriétés du module de reconnaissance partagée.  
  
 Lorsque vous créez un délégué pour un <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> événement, que vous identifiez la méthode qui gérera l’événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant reconnaît des expressions telles que « Affichent la liste des artistes dans la catégorie jazz » ou « Affichent les êtes albums ». L’exemple utilise un gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected> événement pour afficher une notification dans la console lorsque la voix d’entrée ne correspondent pas au contenu de la grammaire avec une confiance suffisante pour produire un module de reconnaissance réussi.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer =  
         new SpeechRecognizer())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=   
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se produit lorsque le module de reconnaissance reçoit l'entrée qui correspond à l'une de ses syntaxes de reconnaissance vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance déclenche le `SpeechRecognized` événement si elle détermine avec une fiabilité suffisante qu’entrée correspond à l’une des grammaires de reconnaissance vocale chargés et activés. Le <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propriété de la <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contient du acceptées <xref:System.Speech.Recognition.RecognitionResult> objet.  
  
 Seuils de niveau de confiance pour le module de reconnaissance partagé, géré par <xref:System.Speech.Recognition.SpeechRecognizer>, sont associés à un profil utilisateur et stocké dans le Registre Windows. Les applications ne doivent pas écrire de modifications dans le Registre pour les propriétés du module de reconnaissance partagée.  
  
 Lorsque le module de reconnaissance reçoit l’entrée qui correspond à une grammaire, le <xref:System.Speech.Recognition.Grammar> objet peut déclencher la <xref:System.Speech.Recognition.Grammar.SpeechRecognized> événement. Le <xref:System.Speech.Recognition.Grammar> l’objet <xref:System.Speech.Recognition.Grammar.SpeechRecognized> événement est déclenché avant la reconnaissance vocale <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> événement.  
  
 Lorsque vous créez un délégué pour un <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> événement, que vous identifiez la méthode qui gérera l’événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant fait partie d’une application console qui charge une syntaxe de reconnaissance vocale et montre la saisie vocale pour le module de reconnaissance partagé, les résultats de reconnaissance associées et les événements associés déclenchés par le module de reconnaissance vocale. Si la reconnaissance vocale Windows n’est pas en cours d’exécution, avant de démarrer cette application sera également démarrer la reconnaissance vocale Windows.  
  
 Parlée entrée comme « Je veux passage de Chicago à Miami » déclenchera un <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> événement. À propos de l’expression « Piloter me de Houston à Chicago » ne déclenche pas une <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> événement.  
  
 L’exemple utilise un gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized> événement pour afficher correctement reconnu des expressions et la sémantique qu’ils contiennent dans la console.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize a shared speech recognition engine.  
    {  
      using (SpeechRecognizer recognizer = new SpeechRecognizer())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="T:System.Speech.Recognition.SpeechRecognizedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected" />
      </Docs>
    </Member>
    <Member MemberName="State">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerState State { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.RecognizerState State" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognizer.State" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property State As RecognizerState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerState State { System::Speech::Recognition::RecognizerState get(); };" />
      <MemberSignature Language="F#" Value="member this.State : System.Speech.Recognition.RecognizerState" Usage="System.Speech.Recognition.SpeechRecognizer.State" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient l'état d'un objet <see cref="T:System.Speech.Recognition.SpeechRecognizer" />.</summary>
        <value>État de l'objet <see langword="SpeechRecognizer" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cette propriété en lecture seule indique si le module de reconnaissance partagé résident dans Windows est dans le `Stopped` ou `Listening` état. Pour plus d’informations, consultez l’énumération <xref:System.Speech.Recognition.RecognizerState>.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />
      </Docs>
    </Member>
    <Member MemberName="StateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; StateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.StateChangedEventArgs&gt; StateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Event StateChanged As EventHandler(Of StateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::StateChangedEventArgs ^&gt; ^ StateChanged;" />
      <MemberSignature Language="F#" Value="member this.StateChanged : EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; " Usage="member this.StateChanged : System.EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.StateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se produit lorsque l'état d'exécution du moteur de reconnaissance de technologie vocale du Bureau Windows change.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance partagé déclenche cet événement lorsque l’état de la reconnaissance vocale Windows le <xref:System.Speech.Recognition.RecognizerState.Listening> ou <xref:System.Speech.Recognition.RecognizerState.Stopped> état.  
  
 Pour obtenir l’état du module de reconnaissance partagée au moment de l’événement, utilisez le <xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A> propriété associé <xref:System.Speech.Recognition.StateChangedEventArgs>. Pour obtenir l’état actuel du moteur de reconnaissance partagée, utilisez le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognizer.State%2A> propriété.  
  
 Lorsque vous créez un délégué pour un <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> événement, que vous identifiez la méthode qui gérera l’événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant crée un module de reconnaissance vocale partagé et crée ensuite deux types de grammaires pour reconnaître des mots spécifiques et pour accepter de dictée gratuite. L’exemple de charge de façon asynchrone toutes les grammaires créés dans le module de reconnaissance.  Un gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> événement utilise la <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> méthode permettant de placer la reconnaissance de Windows en mode « écoute ».  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted += new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized += new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Add a handler for the StateChanged event.  
      recognizer.StateChanged += new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
      // Create "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yah}" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "nah" });  
      SemanticResultValue noValue = new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void  recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
     if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void  recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
     Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void  recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
     string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
      }  
  
      // Add exception handling code here.  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerState" />
        <altmember cref="T:System.Speech.Recognition.StateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.State" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognizer.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Décharge toutes les Grammaires de la Reconnaissance vocale du module de reconnaissance partagé.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si le module de reconnaissance charge actuellement une grammaire de façon asynchrone, cette méthode attend jusqu'à ce que la grammaire est chargée, avant qu’il décharge toutes les grammaires du module de reconnaissance.  
  
 Pour décharger une syntaxe spécifique, utilisez le <xref:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar%2A> (méthode).  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognizer.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognizer.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Grammaire à décharger.</param>
        <summary>Décharge une Grammaire de la Reconnaissance vocale à partir du module de reconnaissance partagé.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si le module de reconnaissance est en cours d’exécution, les applications doivent utiliser <xref:System.Speech.Recognition.SpeechRecognizer.RequestRecognizerUpdate%2A> pour suspendre le moteur de reconnaissance vocale avant le chargement, déchargement, activer ou désactiver une grammaire. Pour décharger toutes les grammaires, utilisez le <xref:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars%2A> (méthode).  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Grammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognizer.UnloadAllGrammars" />
      </Docs>
    </Member>
  </Members>
</Type>