<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata><Meta Name="ms.openlocfilehash" Value="4a68990136b4c4a3e7465b21d6054268587b7843" /><Meta Name="ms.sourcegitcommit" Value="16d2d159872fd213cae4b8f371d7ae9c8b027c89" /><Meta Name="ms.translationtype" Value="HT" /><Meta Name="ms.contentlocale" Value="fr-FR" /><Meta Name="ms.lasthandoff" Value="11/17/2018" /><Meta Name="ms.locfileid" Value="51894595" /></Metadata><TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Fournit le moyen d'accéder et de gérer à un module de reconnaissance vocale in-process.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Vous pouvez créer une instance de cette classe pour tous les modules de reconnaissance vocale installé. Pour obtenir des informations sur les modules de reconnaissance sont installés, utilisez la méthode statique <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> (méthode).  
  
 Cette classe est pour l’exécution de moteurs dans-processus de reconnaissance vocale et fournit un contrôle sur les différents aspects de la reconnaissance vocale, comme suit :  
  
-   Pour créer un module de reconnaissance vocale d’intra-processus, utilisez une de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> constructeurs.  
  
-   Pour gérer le syntaxe de reconnaissance vocale, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> méthodes et le <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> propriété.  
  
-   Pour configurer l’entrée dans le module de reconnaissance, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> (méthode).  
  
-   Pour effectuer la reconnaissance vocale, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> (méthode).  
  
-   Pour modifier comment reconnaissance gère la latence ou entrée inattendue, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriétés.  
  
-   Pour modifier le nombre de variantes retourne le module de reconnaissance, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> propriété. Le module de reconnaissance retourne des résultats de reconnaissance dans un <xref:System.Speech.Recognition.RecognitionResult> objet.  
  
-   Pour synchroniser les modifications dans le module de reconnaissance, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> (méthode). Le module de reconnaissance utilise plusieurs threads pour effectuer des tâches.  
  
-   Pour émuler l’entrée dans le module de reconnaissance, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> méthodes.  
  
 Le <xref:System.Speech.Recognition.SpeechRecognitionEngine> objet est le seul utilisez du processus qui a instancié l’objet. En revanche, le <xref:System.Speech.Recognition.SpeechRecognizer> partage un module de reconnaissance unique avec n’importe quelle application qui souhaite utiliser.  
  
> [!NOTE]
>  Appelez toujours <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> avant de libérer votre dernière référence à la reconnaissance vocale. Sinon, les ressources qu’il utilise ne seront pas libérées tant que le garbage collector n’appelle l’objet de module de reconnaissance `Finalize` (méthode).  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale élémentaire. Étant donné que cet exemple utilise le `Multiple` mode de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> (méthode), il effectue la reconnaissance jusqu'à ce que vous fermez la fenêtre de console ou arrêtez le débogage.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Initialise une nouvelle instance de la classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Vous pouvez construire un <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance à partir d’une des opérations suivantes :  
  
-   Le moteur de reconnaissance vocale par défaut pour le système  
  
-   Un module de reconnaissance vocale spécifique que vous spécifiez par nom  
  
-   Le moteur de reconnaissance vocale par défaut pour les paramètres régionaux que vous spécifiez  
  
-   Un moteur de reconnaissance spécifique qui répond aux critères que vous spécifiez dans un <xref:System.Speech.Recognition.RecognizerInfo> objet.  
  
 Avant de commencer la reconnaissance de la reconnaissance vocale, vous devez charger la grammaire de reconnaissance vocale au moins un et configurer l’entrée pour le module de reconnaissance.  
  
 Pour charger une grammaire, appelez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (méthode).  
  
 Pour configurer l’entrée audio, utilisez une des méthodes suivantes :  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Initialise une nouvelle instance de la classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> à l'aide du mode de reconnaissance vocale par défaut pour le système.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Avant de commencer la reconnaissance vocale, la reconnaissance vocale, vous devez charger au moins une grammaire de reconnaissance et configurer l’entrée pour le module de reconnaissance.  
  
 Pour charger une grammaire, appelez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (méthode).  
  
 Pour configurer l’entrée audio, utilisez une des méthodes suivantes :  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">Paramètres régionaux que le module de reconnaissance vocale doit prendre en charge.</param>
        <summary>Initialise une nouvelle instance de la classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> à l'aide du mode de reconnaissance vocale par défaut pour les paramètres régionaux spécifiés.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows et l’API System.Speech acceptent tous les codes de langue-pays valides. Pour effectuer la reconnaissance vocale à l’aide de la langue spécifiée dans le `CultureInfo` argument, un moteur de reconnaissance vocale qui prend en charge que le code de langue-pays doit être installé. Le module de reconnaissance vocale fournis avec Microsoft Windows 7 fonctionne avec les codes de langue-pays suivants.  
  
-   en-GB. Anglais (Royaume-Uni)  
  
-   en-US. Anglais (États-Unis)  
  
-   fr-fr. Allemand (Allemagne)  
  
-   es-ES. Espagnol (Espagne)  
  
-   fr-FR. Français (France)  
  
-   ja-JP. Japonais (Japon)  
  
-   zh-CN. Chinois (Chine)  
  
-   zh-TW. Chinois (Taïwan)  
  
 Codes de langue à deux lettres tels que « fr », « fr », ou « es » sont également autorisées.  
  
 Avant de commencer la reconnaissance de la reconnaissance vocale, vous devez charger la grammaire de reconnaissance vocale au moins un et configurer l’entrée pour le module de reconnaissance.  
  
 Pour charger une grammaire, appelez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (méthode).  
  
 Pour configurer l’entrée audio, utilisez une des méthodes suivantes :  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale élémentaire et initialise un module de reconnaissance vocale pour les paramètres régionaux en-US.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Aucun des modules de reconnaissance vocale installés prend en charge les paramètres régionaux spécifiés ou la <paramref name="culture" /> est la culture dite indifférente.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="Culture" /> a la valeur <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">Informations sur le module de reconnaissance vocale spécifique.</param>
        <summary>Initialise une nouvelle instance de <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> à l'aide des informations d'un objet <see cref="T:System.Speech.Recognition.RecognizerInfo" /> pour spécifier le module de reconnaissance à utiliser.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Vous pouvez créer une instance de cette classe pour tous les modules de reconnaissance vocale installé. Pour obtenir des informations sur les modules de reconnaissance sont installés, utilisez la <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> (méthode).  
  
 Avant de commencer la reconnaissance de la reconnaissance vocale, vous devez charger la grammaire de reconnaissance vocale au moins un et configurer l’entrée pour le module de reconnaissance.  
  
 Pour charger une grammaire, appelez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (méthode).  
  
 Pour configurer l’entrée audio, utilisez une des méthodes suivantes :  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale élémentaire et initialise un module de reconnaissance vocale qui prend en charge de la langue anglaise.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">Nom de jeton du module de reconnaissance vocale à utiliser.</param>
        <summary>Initialise une nouvelle instance de la classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> avec un paramètre de chaîne qui spécifie le nom du module de reconnaissance à utiliser.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le nom du jeton du module de reconnaissance est la valeur de la <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> propriété de la <xref:System.Speech.Recognition.RecognizerInfo> objet retourné par la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> propriété du module de reconnaissance. Pour obtenir une collection de tous les modules de reconnaissance installés, utilisez la méthode statique <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> (méthode).  
  
 Avant de commencer la reconnaissance de la reconnaissance vocale, vous devez charger la grammaire de reconnaissance vocale au moins un et configurer l’entrée pour le module de reconnaissance.  
  
 Pour charger une grammaire, appelez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (méthode).  
  
 Pour configurer l’entrée audio, utilisez une des méthodes suivantes :  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale élémentaire et crée une instance de la 8.0 de module de reconnaissance vocale pour Windows (anglais - US).  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Aucun module de reconnaissance vocale installé avec ce nom de jeton ou l'<paramref name="recognizerId" /> est la chaîne vide ("").</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="recognizerId" /> a la valeur <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient le format de l'audio reçu par le <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Le format de l'audio au niveau de l'entrée de l'instance <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> , ou <see langword="null" /> si l'entrée n'est pas configurée ou est définie sur l'entrée null.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Pour configurer l’entrée audio, utilisez une des méthodes suivantes :  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 L’exemple ci-dessous utilise <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> pour obtenir et afficher des données au format audio.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient le niveau de l'audio reçu par le <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Niveau sonore de l'entrée fournie au module de reconnaissance vocale, compris entre 0 et 100.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 La valeur 0 représente la latence, et 100 représente le volume d’entrée maximal.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Déclenché lorsque <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> stocke le niveau de son entrée audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le <xref:System.Speech.Recognition.SpeechRecognitionEngine> déclenche cet événement plusieurs fois par seconde. La fréquence avec laquelle l’événement est déclenché dépend de l’ordinateur sur lequel l’application est en cours d’exécution.  
  
 Pour obtenir le niveau audio au moment de l’événement, utilisez le <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> propriété associé <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Pour obtenir le niveau audio actuel de l’entrée dans le module de reconnaissance, utilisez le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> propriété.  
  
 Lorsque vous créez un délégué <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated>, vous identifiez la méthode qui gérera l'événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant ajoute un gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> événement à un <xref:System.Speech.Recognition.SpeechRecognitionEngine> objet. Le gestionnaire génère le nouveau niveau audio à la console.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient la position actuelle dans le flux audio généré par le périphérique qui gère les entrées pour <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Emplacement actuel dans le flux audio généré par le périphérique d'entrée.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propriété référence la position du périphérique d’entrée dans son flux audio généré. En revanche, le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> propriété référence la position du module de reconnaissance dans son entrée audio. Ces positions peuvent être différentes. Par exemple, si le module de reconnaissance a reçu d’entrée pour lesquels il n’a pas encore généré un résultat de reconnaissance ensuite la valeur de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> propriété est inférieure à la valeur de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propriété.  
  
   
  
## Examples  
 Dans l’exemple suivant, le module de reconnaissance vocale de dans le processus utilise une syntaxe de dictée pour faire correspondre la saisie vocale. Un gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> événement écrit dans la console le <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> lorsque le module de reconnaissance vocale détecte une reconnaissance vocale à son entrée.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Déclenché lorsque <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> détecte un problème dans le signal audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Pour obtenir le problème s’est produit, utilisez le <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> propriété associé <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Lorsque vous créez un délégué <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred>, vous identifiez la méthode qui gérera l'événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant définit un gestionnaire d’événements qui rassemble des informations sur un <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> événement.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient l'état de l'audio reçu par le <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>État de l'entrée audio dans le module de reconnaissance vocale.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> propriété représente l’état audio avec un membre de la <xref:System.Speech.Recognition.AudioState> énumération.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Déclenché lorsque l'état audio reçu par le <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> change.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Pour obtenir l’état audio au moment de l’événement, utilisez le <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> propriété associé <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Pour obtenir l’état audio actuel de l’entrée dans le module de reconnaissance, utilisez le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> propriété. Pour plus d’informations sur l’état audio, consultez le <xref:System.Speech.Recognition.AudioState> énumération.  
  
 Lorsque vous créez un délégué <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>, vous identifiez la méthode qui gérera l'événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant utilise un gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> événement à écrire le module de reconnaissance du nouveau <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> à la console chaque fois que cela change, à l’aide d’un membre de la <xref:System.Speech.Recognition.AudioState> énumération.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient ou définit l'intervalle de temps pendant lequel <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> accepte une entrée contenant uniquement un son d'arrière-plan, avant la finalisation de la reconnaissance.</summary>
        <value>Durée de l'intervalle.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Chaque module de reconnaissance vocale dispose d’un algorithme pour faire la distinction entre la latence et de reconnaissance vocale. Le module de reconnaissance classifie qu’aucun silence non-d’entrée qui ne correspond pas à la règle initiale d’un du module de reconnaissance de bruit de fond chargé et syntaxe de reconnaissance vocale est activée. Si le module de reconnaissance reçoit uniquement le bruit de fond et silence dans l’intervalle de délai d’attente de rumeur a, puis le module de reconnaissance finalise ce module de reconnaissance.  
  
-   Pour les opérations de reconnaissance asynchrone, le module de reconnaissance déclenche le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> événement, où le <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> propriété est `true`et le <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> propriété est `null`.  
  
-   Pour les opérations de reconnaissance synchrone et émulation, retourne le module de reconnaissance `null`, au lieu d’une commande valide <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Si la période de délai d’attente de rumeur a est définie sur 0, le module de reconnaissance n’effectue pas une vérification de délai d’attente de rumeur a. L’intervalle de délai d’attente peut être n’importe quelle valeur non négative. La valeur par défaut est 0 seconde.  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale élémentaire qui définit le <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> et <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriétés d’un <xref:System.Speech.Recognition.SpeechRecognitionEngine> avant de lancer la reconnaissance vocale. Gestionnaires pour le module de reconnaissance vocale <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> et <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> événements des informations sur la console pour illustrer les événements de sortie comment les <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriétés d’un <xref:System.Speech.Recognition.SpeechRecognitionEngine> affectent les opérations de reconnaissance.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Cette propriété a une valeur inférieure à 0 seconde.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Supprime l'objet <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Supprime l'objet <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing"><see langword="true" /> pour libérer les ressources managées et non managées ; <see langword="false" /> pour ne libérer que les ressources non managées.</param>
        <summary>Supprime l’objet <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> et libère les ressources utilisées pendant la session.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Émule l'entrée dans le module de reconnaissance vocale, en utilisant le texte à la place de l'audio pour la reconnaissance vocale synchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ces méthodes contourner l’entrée audio du système et saisissez un texte dans le module de reconnaissance en tant que <xref:System.String> objets ou sous forme de tableau de <xref:System.Speech.Recognition.RecognizedWordUnit> objets. Cela peut être utile lorsque vous testez ou le débogage d’une application ou grammaire. Par exemple, vous pouvez utiliser l’émulation pour déterminer si un mot dans une syntaxe et les sémantiques sont retournés lorsque le mot est reconnu. Utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> méthode permettant de désactiver l’entrée audio pour le moteur de reconnaissance vocale pendant les opérations d’émulation.  
  
 Le déclenche de module de reconnaissance vocale le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événements comme si l’opération de reconnaissance n’est pas émulée. Le module de reconnaissance ignore les nouvelles lignes et l’espace blanc superflu et traite des signes de ponctuation en tant qu’entrée littéral.  
  
> [!NOTE]
>  Le <xref:System.Speech.Recognition.RecognitionResult> objet généré par le module de reconnaissance vocale en réponse à l’entrée émulée a la valeur `null` pour son <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> propriété.  
  
 Pour émuler le module de reconnaissance asynchrone, utilisez la <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> (méthode).  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Entrée de l'opération de reconnaissance.</param>
        <summary>Émule l'entrée d'une expression dans le module de reconnaissance vocale, en utilisant le texte à la place de l'audio pour la reconnaissance vocale synchrone.</summary>
        <returns>Résultat de l'opération de reconnaissance ou <see langword="null" /> si l'opération n'est pas réussie ou si le module de reconnaissance n'est pas activé.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le déclenche de module de reconnaissance vocale le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événements comme si l’opération de reconnaissance n’est pas émulée.  
  
 Les modules de reconnaissance fournis avec Vista et Windows 7 ignorent la casse et la largeur de caractères lors de l’application des règles de grammaire à l’expression d’entrée. Pour plus d’informations sur ce type de comparaison, consultez le <xref:System.Globalization.CompareOptions> valeurs d’énumération <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> et <xref:System.Globalization.CompareOptions.IgnoreWidth>. Les modules de reconnaissance également ignorent les nouvelles lignes et les espaces superflus et traitent des signes de ponctuation en tant qu’entrée littéral.  
  
   
  
## Examples  
 L’exemple de code suivant fait partie d’une application console qui illustre l’entrée émulée, les résultats de reconnaissance associées et les événements associés déclenchés par le module de reconnaissance vocale. L’exemple génère la sortie suivante.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Le module de reconnaissance n'a aucune syntaxe de reconnaissance vocale chargée.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> a la valeur <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> est la chaîne vide ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Un tableau d'unités de mot qui contient l'entrée pour l'opération de reconnaissance.</param>
        <param name="compareOptions">Combinaison d'opérations de bits des valeurs d'énumération qui décrivent le type de comparaison à utiliser pour la reconnaissance émulée.</param>
        <summary>Émule l'entrée de mots spécifiques dans le module de reconnaissance vocale partagé, en utilisant le texte à la place de l'audio pour la reconnaissance vocale synchrone, et spécifie comment le module de reconnaissance gère la comparaison Unicode entre les mots et les grammaires de la reconnaissance vocale chargées.</summary>
        <returns>Résultat de l'opération de reconnaissance ou <see langword="null" /> si l'opération n'est pas réussie ou si le module de reconnaissance n'est pas activé.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le déclenche de module de reconnaissance vocale le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événements comme si l’opération de reconnaissance n’est pas émulée.  
  
 Utilise le module de reconnaissance `compareOptions` quand elle s’applique les règles de grammaire à l’expression d’entrée. Les modules de reconnaissance fournis avec Vista et Windows 7 ignorent la casse si le <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> ou <xref:System.Globalization.CompareOptions.IgnoreCase> valeur est présente. Le module de reconnaissance ignore toujours la largeur des caractères et jamais ignore le type Kana. Le module de reconnaissance également ignore les nouvelles lignes et l’espace blanc superflu et traite des signes de ponctuation en tant qu’entrée littéral. Pour plus d’informations sur la largeur des caractères et le type Kana, consultez le <xref:System.Globalization.CompareOptions> énumération.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Le module de reconnaissance n'a aucune syntaxe de reconnaissance vocale chargée.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" /> a la valeur <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" /> contient un ou plusieurs éléments <see langword="null" />.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> contient l'indicateur <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> ou <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Expression d'entrée de l'opération de reconnaissance.</param>
        <param name="compareOptions">Combinaison d'opérations de bits des valeurs d'énumération qui décrivent le type de comparaison à utiliser pour la reconnaissance émulée.</param>
        <summary>Émule l'entrée d'une expression dans le module de reconnaissance vocale partagé, en utilisant le texte à la place de l'audio pour la reconnaissance vocale synchrone, et spécifie comment le module de reconnaissance gère la comparaison Unicode entre l'expression et les grammaires de la reconnaissance vocale chargées.</summary>
        <returns>Résultat de l'opération de reconnaissance ou <see langword="null" /> si l'opération n'est pas réussie ou si le module de reconnaissance n'est pas activé.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le déclenche de module de reconnaissance vocale le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événements comme si l’opération de reconnaissance n’est pas émulée.  
  
 Utilise le module de reconnaissance `compareOptions` quand elle s’applique les règles de grammaire à l’expression d’entrée. Les modules de reconnaissance fournis avec Vista et Windows 7 ignorent la casse si le <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> ou <xref:System.Globalization.CompareOptions.IgnoreCase> valeur est présente. Le module de reconnaissance ignore toujours la largeur des caractères et jamais ignore le type Kana. Le module de reconnaissance également ignore les nouvelles lignes et l’espace blanc superflu et traite des signes de ponctuation en tant qu’entrée littéral. Pour plus d’informations sur la largeur des caractères et le type Kana, consultez le <xref:System.Globalization.CompareOptions> énumération.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Le module de reconnaissance n'a aucune syntaxe de reconnaissance vocale chargée.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> a la valeur <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> est la chaîne vide ("").</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> contient l'indicateur <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> ou <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Émule l'entrée dans le module de reconnaissance vocale, en utilisant le texte à la place de l'audio pour la reconnaissance vocale asynchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ces méthodes contourner l’entrée audio du système et saisissez un texte dans le module de reconnaissance en tant que <xref:System.String> objets ou sous forme de tableau de <xref:System.Speech.Recognition.RecognizedWordUnit> objets. Cela peut être utile lorsque vous testez ou le débogage d’une application ou grammaire. Par exemple, vous pouvez utiliser l’émulation pour déterminer si un mot dans une syntaxe et les sémantiques sont retournés lorsque le mot est reconnu. Utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> méthode permettant de désactiver l’entrée audio pour le moteur de reconnaissance vocale pendant les opérations d’émulation.  
  
 Le déclenche de module de reconnaissance vocale le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événements comme si l’opération de reconnaissance n’est pas émulée. Lorsque le module de reconnaissance termine l’opération de reconnaissance asynchrone, il déclenche le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> événement. Le module de reconnaissance ignore les nouvelles lignes et l’espace blanc superflu et traite des signes de ponctuation en tant qu’entrée littéral.  
  
> [!NOTE]
>  Le <xref:System.Speech.Recognition.RecognitionResult> objet généré par le module de reconnaissance vocale en réponse à l’entrée émulée a la valeur `null` pour son <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> propriété.  
  
 Pour émuler la reconnaissance synchrone, utilisez la <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> (méthode).  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Entrée de l'opération de reconnaissance.</param>
        <summary>Émule l'entrée d'une expression dans le module de reconnaissance vocale, en utilisant le texte à la place de l'audio pour la reconnaissance vocale asynchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le déclenche de module de reconnaissance vocale le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événements comme si l’opération de reconnaissance n’est pas émulée. Lorsque le module de reconnaissance termine l’opération de reconnaissance asynchrone, il déclenche le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> événement.  
  
 Les modules de reconnaissance fournis avec Vista et Windows 7 ignorent la casse et la largeur de caractères lors de l’application des règles de grammaire à l’expression d’entrée. Pour plus d’informations sur ce type de comparaison, consultez le <xref:System.Globalization.CompareOptions> valeurs d’énumération <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> et <xref:System.Globalization.CompareOptions.IgnoreWidth>. Les modules de reconnaissance également ignorent les nouvelles lignes et les espaces superflus et traitent des signes de ponctuation en tant qu’entrée littéral.  
  
   
  
## Examples  
 L’exemple de code suivant fait partie d’une application console qui illustre l’entrée émulée asynchrone, les résultats de reconnaissance associées et les événements associés déclenchés par le module de reconnaissance vocale. L’exemple génère la sortie suivante.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHander);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHander(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Le module de reconnaissance n'a aucune syntaxe de reconnaissance vocale chargée, ou le module de reconnaissance a une opération de reconnaissance asynchrone qui n'est pas encore terminée.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> a la valeur <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> est la chaîne vide ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Un tableau d'unités de mot qui contient l'entrée pour l'opération de reconnaissance.</param>
        <param name="compareOptions">Combinaison d'opérations de bits des valeurs d'énumération qui décrivent le type de comparaison à utiliser pour la reconnaissance émulée.</param>
        <summary>Émule l'entrée de mots spécifiques dans le module de reconnaissance vocale partagé, en utilisant un tableau d'objets <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> à la place de l'audio pour la reconnaissance vocale synchrone, et spécifie comment le module de reconnaissance gère la comparaison Unicode entre les mots et les grammaires de la reconnaissance vocale chargées.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le déclenche de module de reconnaissance vocale le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événements comme si l’opération de reconnaissance n’est pas émulée. Lorsque le module de reconnaissance termine l’opération de reconnaissance asynchrone, il déclenche le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> événement.  
  
 Utilise le module de reconnaissance `compareOptions` quand elle s’applique les règles de grammaire à l’expression d’entrée. Les modules de reconnaissance fournis avec Vista et Windows 7 ignorent la casse si le <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> ou <xref:System.Globalization.CompareOptions.IgnoreCase> valeur est présente. Les modules de reconnaissance toujours ignorent la largeur des caractères et jamais ignorent le type Kana. Les modules de reconnaissance également ignorent les nouvelles lignes et les espaces superflus et traitent des signes de ponctuation en tant qu’entrée littéral. Pour plus d’informations sur la largeur des caractères et le type Kana, consultez le <xref:System.Globalization.CompareOptions> énumération.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Le module de reconnaissance n'a aucune syntaxe de reconnaissance vocale chargée, ou le module de reconnaissance a une opération de reconnaissance asynchrone qui n'est pas encore terminée.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" /> a la valeur <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" /> contient un ou plusieurs éléments <see langword="null" />.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> contient l'indicateur <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> ou <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Expression d'entrée de l'opération de reconnaissance.</param>
        <param name="compareOptions">Combinaison d'opérations de bits des valeurs d'énumération qui décrivent le type de comparaison à utiliser pour la reconnaissance émulée.</param>
        <summary>Émule l'entrée d'une expression dans le module de reconnaissance vocale partagé, en utilisant le texte à la place de l'audio pour la reconnaissance vocale asynchrone, et spécifie comment le module de reconnaissance gère la comparaison Unicode entre l'expression et les grammaires de la reconnaissance vocale chargées.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le déclenche de module de reconnaissance vocale le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événements comme si l’opération de reconnaissance n’est pas émulée. Lorsque le module de reconnaissance termine l’opération de reconnaissance asynchrone, il déclenche le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> événement.  
  
 Utilise le module de reconnaissance `compareOptions` quand elle s’applique les règles de grammaire à l’expression d’entrée. Les modules de reconnaissance fournis avec Vista et Windows 7 ignorent la casse si le <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> ou <xref:System.Globalization.CompareOptions.IgnoreCase> valeur est présente. Les modules de reconnaissance toujours ignorent la largeur des caractères et jamais ignorent le type Kana. Les modules de reconnaissance également ignorent les nouvelles lignes et les espaces superflus et traitent des signes de ponctuation en tant qu’entrée littéral. Pour plus d’informations sur la largeur des caractères et le type Kana, consultez le <xref:System.Globalization.CompareOptions> énumération.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Le module de reconnaissance n'a aucune syntaxe de reconnaissance vocale chargée, ou le module de reconnaissance a une opération de reconnaissance asynchrone qui n'est pas encore terminée.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> a la valeur <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> est la chaîne vide ("").</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> contient l'indicateur <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> ou <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Déclenché lorsque <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> finalise un module de reconnaissance asynchrone d'entrée émulée.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Chaque <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> méthode commence une opération de reconnaissance asynchrone. Le <xref:System.Speech.Recognition.SpeechRecognitionEngine> déclenche le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> événement lorsqu’il finalise l’opération asynchrone.  
  
 Le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> opération peut déclencher la <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événements. Le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> événement est le dernier événement de ce type que le module de reconnaissance se déclenche pour une opération donnée.  
  
 Si la reconnaissance émulée a réussi, vous pouvez accéder le résultat de la reconnaissance à l’aide de l’une des opérations suivantes :  
  
-   Le <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> propriété dans le <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> objet dans le gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> événement.  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propriété dans le <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> objet dans le gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événement.  
  
 Si la reconnaissance émulée n’a pas réussie, le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événement n’est pas déclenché et le <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> sera null.  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> dérive de <xref:System.ComponentModel.AsyncCompletedEventArgs>.  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> dérive de <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Lorsque vous créez un délégué <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>, vous identifiez la méthode qui gérera l'événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant fait partie d’une application console qui charge une syntaxe de reconnaissance vocale et illustre l’entrée émulée asynchrone, les résultats de reconnaissance associées et les événements associés déclenchés par le module de reconnaissance vocale.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call mathches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient ou définit l'intervalle de silence que <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> acceptera à la fin de l'entrée sans ambiguïté avant de finaliser une opération de reconnaissance.</summary>
        <value>Durée de l'intervalle de silence.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance vocale utilise cet intervalle de délai d’expiration lors de l’entrée de reconnaissance est sans équivoque. Par exemple, pour une grammaire de reconnaissance vocale qui prend en charge la reconnaissance de le » nouvelle de jeux, » ou « nouvelle partie, » » nouvelle, de jeux » est une entrée non équivoque, et « nouvelle game » est une entrée ambiguë.  
  
 Cette propriété détermine la durée pendant laquelle le moteur de reconnaissance vocale attendra une entrée supplémentaire avant de finaliser une opération de reconnaissance. L’intervalle de délai peut provenir de 0 secondes à 10 secondes, inclusifs. La valeur par défaut est 150 millisecondes.  
  
 Pour définir l’intervalle de délai d’expiration pour l’entrée ambiguë, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriété.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Cette propriété a une valeur inférieure à 0 ou supérieure à 10 secondes.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient ou définit l'intervalle de silence que <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> acceptera à la fin de l'entrée ambiguë avant de finaliser une opération de reconnaissance.</summary>
        <value>Durée de l'intervalle de silence.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance vocale utilise cet intervalle de délai d’expiration lors de l’entrée de reconnaissance est AMBIGUE. Par exemple, pour une grammaire de reconnaissance vocale qui prend en charge la reconnaissance de le » nouvelle de jeux, » ou « nouvelle partie, » » nouvelle, de jeux » est une entrée non équivoque, et « nouvelle game » est une entrée ambiguë.  
  
 Cette propriété détermine la durée pendant laquelle le moteur de reconnaissance vocale attendra une entrée supplémentaire avant de finaliser une opération de reconnaissance. L’intervalle de délai peut provenir de 0 secondes à 10 secondes, inclusifs. La valeur par défaut est 500 millisecondes.  
  
 Pour définir l’intervalle de délai d’expiration pour l’entrée sans ambiguïté, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> propriété.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Cette propriété a une valeur inférieure à 0 ou supérieure à 10 secondes.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient une collection des objets <see cref="T:System.Speech.Recognition.Grammar" /> chargés dans cette instance <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Collection d’objets <see cref="T:System.Speech.Recognition.Grammar" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 L’exemple suivant génère des informations sur la console pour chaque syntaxe de reconnaissance vocale est actuellement chargé par un module de reconnaissance vocale.  
  
> [!IMPORTANT]
>  Copier la collection de grammaire pour éviter les erreurs si la collection est modifiée pendant que cette méthode énumère les éléments de la collection.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0;netframework-4.5;netframework-4.5.1;netframework-4.5.2;netframework-4.6;netframework-4.6.1;netframework-4.6.2;netframework-4.7;netframework-4.7.1;netframework-4.7.2;netframework-4.8">
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
        <Attribute FrameworkAlternate="netframework-3.0;netframework-3.5">
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient ou définit l'intervalle de temps pendant lequel <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> accepte une entrée contenant uniquement un silence, avant la finalisation de la reconnaissance.</summary>
        <value>Durée de l'intervalle de silence.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Chaque module de reconnaissance vocale dispose d’un algorithme pour faire la distinction entre la latence et de reconnaissance vocale. Si l’entrée de module de reconnaissance est silence pendant la période de délai d’attente de silence initial, puis le module de reconnaissance finalise ce module de reconnaissance.  
  
-   Pour les opérations de reconnaissance asynchrone et émulation, déclenche le module de reconnaissance le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> événement, où le <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> propriété est `true`et le <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> propriété est `null`.  
  
-   Pour les opérations de reconnaissance synchrone et émulation, retourne le module de reconnaissance `null`, au lieu d’une commande valide <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Si l’intervalle de délai d’attente de silence initial est définie sur 0, le module de reconnaissance n’effectue pas une vérification de délai d’attente de silence initial. L’intervalle de délai d’attente peut être n’importe quelle valeur non négative. La valeur par défaut est 0 seconde.  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale élémentaire. L’exemple définit le <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> et <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriétés d’un <xref:System.Speech.Recognition.SpeechRecognitionEngine> avant de lancer la reconnaissance vocale. Gestionnaires pour le module de reconnaissance vocale <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> et <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> événements de sortie des informations d’événement dans la console pour illustrer comment la <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriétés d’un <xref:System.Speech.Recognition.SpeechRecognitionEngine> propriétés affectent les opérations de reconnaissance.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Cette propriété a une valeur inférieure à 0 seconde.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Retourne les informations sur tous les modules de reconnaissance vocale installés sur le système actuel.</summary>
        <returns>Collection d'objets <see cref="T:System.Speech.Recognition.RecognizerInfo" /> en lecture seule qui décrivent les modules de reconnaissance installés.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Pour obtenir des informations sur le module de reconnaissance actuel, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> propriété.  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale élémentaire. L’exemple utilise la collection retournée par la <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> méthode pour rechercher un module de reconnaissance vocale qui prend en charge de la langue anglaise.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Objet de grammaire à charger.</param>
        <summary>Charge un objet <see cref="T:System.Speech.Recognition.Grammar" /> de façon synchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance lève une exception si le <xref:System.Speech.Recognition.Grammar> objet est déjà chargé, est en cours de chargement en mode asynchrone ou n’a pas pu charger dans n’importe quel module de reconnaissance. Vous ne pouvez pas charger le même <xref:System.Speech.Recognition.Grammar> objet dans plusieurs instances de <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Au lieu de cela, créez un <xref:System.Speech.Recognition.Grammar> pour chaque objet <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.  
  
 Si le module de reconnaissance est en cours d’exécution, les applications doivent utiliser <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> pour suspendre le moteur de reconnaissance vocale avant le chargement, déchargement, activer ou désactiver une grammaire.  
  
 Lorsque vous chargez une grammaire, il est activé par défaut. Pour désactiver une grammaire chargée, utilisez le <xref:System.Speech.Recognition.Grammar.Enabled%2A> propriété.  
  
 Pour charger un <xref:System.Speech.Recognition.Grammar> objet de façon asynchrone, utilisez la <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (méthode).  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale élémentaire. L’exemple crée un <xref:System.Speech.Recognition.DictationGrammar> et les charge dans un module de reconnaissance vocale.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> a la valeur <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" /> n'est pas un état valide.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Syntaxe de reconnaissance vocale à charger.</param>
        <summary>Charge une syntaxe de reconnaissance vocale de façon asynchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Lorsque le module de reconnaissance termine le chargement un <xref:System.Speech.Recognition.Grammar> de l’objet, il déclenche une <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> événement. Le module de reconnaissance lève une exception si le <xref:System.Speech.Recognition.Grammar> objet est déjà chargé, est en cours de chargement en mode asynchrone ou n’a pas pu charger dans n’importe quel module de reconnaissance. Vous ne pouvez pas charger le même <xref:System.Speech.Recognition.Grammar> objet dans plusieurs instances de <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Au lieu de cela, créez un <xref:System.Speech.Recognition.Grammar> pour chaque objet <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.  
  
 Si le module de reconnaissance est en cours d’exécution, les applications doivent utiliser <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> pour suspendre le moteur de reconnaissance vocale avant le chargement, déchargement, activer ou désactiver une grammaire.  
  
 Lorsque vous chargez une grammaire, il est activé par défaut. Pour désactiver une grammaire chargée, utilisez le <xref:System.Speech.Recognition.Grammar.Enabled%2A> propriété.  
  
 Pour charger une grammaire de reconnaissance vocale de façon synchrone, utilisez la <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> (méthode).  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> a la valeur <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" /> n'est pas un état valide.</exception>
        <exception cref="T:System.OperationCanceledException">L'opération d'annulation asynchrone a été annulée.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Déclenché lorsque <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> termine le chargement asynchrone d'un objet <see cref="T:System.Speech.Recognition.Grammar" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> méthode lance une opération asynchrone. Le <xref:System.Speech.Recognition.SpeechRecognitionEngine> déclenche cet événement lorsqu’il termine l’opération. Pour obtenir le <xref:System.Speech.Recognition.Grammar> que le module de reconnaissance est chargé de l’objet, utilisez le <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> propriété associé <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Pour obtenir des cours <xref:System.Speech.Recognition.Grammar> objets le module de reconnaissance a chargé, utilisez le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> propriété.  
  
 Si le module de reconnaissance est en cours d’exécution, les applications doivent utiliser <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> pour suspendre le moteur de reconnaissance vocale avant le chargement, déchargement, activer ou désactiver une grammaire.  
  
 Lorsque vous créez un délégué <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>, vous identifiez la méthode qui gérera l'événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant crée un module de reconnaissance vocale d’intra-processus et crée ensuite deux types de grammaires pour reconnaître des mots spécifiques et pour accepter de dictée gratuite. L’exemple construit un <xref:System.Speech.Recognition.Grammar> objet à partir de chacun des grammaires de reconnaissance vocale terminée, puis charge de façon asynchrone le <xref:System.Speech.Recognition.Grammar> des objets sur le <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance. Gestionnaires pour le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> et <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> écrire des événements dans la console le nom de la <xref:System.Speech.Recognition.Grammar> objet qui a été utilisé pour effectuer la reconnaissance et le texte de résultat de reconnaissance, respectivement.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient ou définit le nombre maximal de résultats de reconnaissance de substitution retournés pour chaque opération de reconnaissance par le <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Nombre d'autres résultats à retourner.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> propriété de la <xref:System.Speech.Recognition.RecognitionResult> classe contient la collection de <xref:System.Speech.Recognition.RecognizedPhrase> objets qui représentent des interprétations possibles de l’entrée.  
  
 La valeur par défaut <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> est 10.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException"><see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" /> a une valeur inférieure à 0.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Nom du paramètre à retourner.</param>
        <summary>Retourne les valeurs des paramètres pour le module de reconnaissance.</summary>
        <returns>La valeur du paramètre.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Paramètres de module de reconnaissance peuvent contenir des données de chaîne, entier 64 bits ou mémoire adresse. Le tableau suivant décrit les paramètres qui sont définis pour une API Microsoft Speech (SAPI)-module de reconnaissance conforme. Les paramètres suivants doivent avoir la même plage pour chaque module de reconnaissance qui prend en charge le paramètre. Un module de reconnaissance vocale compatible SAPI n’est pas nécessaire pour prendre en charge ces paramètres et peut prendre en charge les autres paramètres.  
  
|Name|Description |  
|----------|-----------------|  
|`ResourceUsage`|Spécifie la consommation du processeur du module de reconnaissance. La plage est comprise entre 0 et 100. La valeur par défaut est 50.|  
|`ResponseSpeed`|Indique la longueur de silence à la fin de l’entrée sans ambiguïté avant que le module de reconnaissance vocale termine une opération de reconnaissance. La plage est comprise entre 0 et 10 000 millisecondes (ms). Ce paramètre correspond à la reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> propriété.  Par défaut = 150ms.|  
|`ComplexResponseSpeed`|Indique la longueur de silence à la fin de l’entrée ambiguë avant que le module de reconnaissance vocale termine une opération de reconnaissance. La plage est comprise entre 0 à 10 000 ms. Ce paramètre correspond à la reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriété. Par défaut = 500 ms.|  
|`AdaptationOn`|Indique si une adaptation du modèle acoustique est activé (valeur = `1`) ou OFF (valeur = `0`). La valeur par défaut est `1` (ON).|  
|`PersistedBackgroundAdaptation`|Indique si une adaptation en arrière-plan est activé (valeur = `1`) ou OFF (valeur = `0`), et conserve le paramètre dans le Registre. La valeur par défaut est `1` (ON).|  
  
 Pour mettre à jour un paramètre pour le module de reconnaissance, utilisez une de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> méthodes.  
  
   
  
## Examples  
 L’exemple suivant fait partie d’une application console qui génère les valeurs pour un nombre de paramètres définis pour le module de reconnaissance qui prend en charge les paramètres régionaux en-US. L’exemple génère la sortie suivante.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> a la valeur <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> est la chaîne vide ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Le module de reconnaissance n'a pas de paramètre correspondant à ce nom.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Démarre une opération de reconnaissance vocale synchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ces méthodes effectuent une opération de reconnaissance unique, synchrone. Le module de reconnaissance effectue cette opération sur sa syntaxe de reconnaissance vocale chargés et activés.  
  
 Pendant un appel à cette méthode, le module de reconnaissance peut déclencher des événements suivants :  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Déclenché lorsque le module de reconnaissance détecte l’entrée qu’il peut identifier comme vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Déclenché lorsque l’entrée crée une correspondance ambiguë avec l’une des grammaires actives.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Déclenché lorsque le module de reconnaissance finalise un module de reconnaissance.  
  
 Le module de reconnaissance ne déclenche pas le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> événement lorsque l’un de le <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> méthodes.  
  
 Le <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> méthodes retournent un <xref:System.Speech.Recognition.RecognitionResult> objet, ou `null` si l’opération n’a pas réussi, ou le module de reconnaissance n’est pas activé.  
  
 Une opération de reconnaissance synchrone peut échouer pour les raisons suivantes :  
  
-   Reconnaissance vocale n’est pas détecté avant l’expirent des intervalles de délai d’attente pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriétés, ou pour le `initialSilenceTimeout` paramètre de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> (méthode).  
  
-   Le moteur de reconnaissance vocale de détecte, mais ne trouve aucune correspondance dans un de ses chargés et activés <xref:System.Speech.Recognition.Grammar> objets.  
  
 Pour modifier la façon dont le module de reconnaissance gère le minutage de reconnaissance vocale ou silence en ce qui concerne la reconnaissance, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriétés.  
  
 Le <xref:System.Speech.Recognition.SpeechRecognitionEngine> doit avoir au moins un <xref:System.Speech.Recognition.Grammar> objet chargé avant d’effectuer la reconnaissance. Pour charger une grammaire de reconnaissance vocale, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (méthode).  
  
 Pour effectuer la reconnaissance asynchrone, utilisez une de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> méthodes.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Effectue une opération de reconnaissance vocale synchrone.</summary>
        <returns>Résultat de la reconnaissance pour l'entrée ou <see langword="null" /> si l'opération n'est pas réussie ou que le module de reconnaissance n'est pas activé.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cette méthode effectue une opération de reconnaissance unique. Le module de reconnaissance effectue cette opération sur sa syntaxe de reconnaissance vocale chargés et activés.  
  
 Pendant un appel à cette méthode, le module de reconnaissance peut déclencher des événements suivants :  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Déclenché lorsque le module de reconnaissance détecte l’entrée qu’il peut identifier comme vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Déclenché lorsque l’entrée crée une correspondance ambiguë avec l’une des grammaires actives.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Déclenché lorsque le module de reconnaissance finalise un module de reconnaissance.  
  
 Le module de reconnaissance ne déclenche pas le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> événement lors de l’utilisation de cette méthode.  
  
 Le <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> méthode retourne un <xref:System.Speech.Recognition.RecognitionResult> objet, ou `null` si l’opération n’a pas réussi.  
  
 Une opération de reconnaissance synchrone peut échouer pour les raisons suivantes :  
  
-   Reconnaissance vocale n’est pas détecté avant l’expirent des intervalles de délai d’attente pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriétés.  
  
-   Le moteur de reconnaissance vocale de détecte, mais ne trouve aucune correspondance dans un de ses chargés et activés <xref:System.Speech.Recognition.Grammar> objets.  
  
 Pour effectuer la reconnaissance asynchrone, utilisez une de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> méthodes.  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale élémentaire. L’exemple crée un <xref:System.Speech.Recognition.DictationGrammar>, il charge dans un module de reconnaissance vocale d’intra-processus et effectue une opération de reconnaissance.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">Intervalle de temps pendant lequel un module de reconnaissance vocale accepte une entrée contenant uniquement un silence, avant la finalisation de la reconnaissance.</param>
        <summary>Effectue une opération de reconnaissance vocale synchrone avec un délai d'attente de silence initial spécifié.</summary>
        <returns>Résultat de la reconnaissance pour l'entrée ou <see langword="null" /> si l'opération n'est pas réussie ou que le module de reconnaissance n'est pas activé.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si le moteur de reconnaissance vocale détecte vocale au sein de l’intervalle de temps spécifié par `initialSilenceTimeout` argument, <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> effectue une opération de reconnaissance unique, puis termine.  Le `initialSilenceTimeout` paramètre remplace le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriété.  
  
 Pendant un appel à cette méthode, le module de reconnaissance peut déclencher des événements suivants :  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Déclenché lorsque le module de reconnaissance détecte l’entrée qu’il peut identifier comme vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Déclenché lorsque l’entrée crée une correspondance ambiguë avec l’une des grammaires actives.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Déclenché lorsque le module de reconnaissance finalise un module de reconnaissance.  
  
 Le module de reconnaissance ne déclenche pas le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> événement lors de l’utilisation de cette méthode.  
  
 Le <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> méthode retourne un <xref:System.Speech.Recognition.RecognitionResult> objet, ou `null` si l’opération n’a pas réussi.  
  
 Une opération de reconnaissance synchrone peut échouer pour les raisons suivantes :  
  
-   Reconnaissance vocale n’est pas détecté avant l’expirent des intervalles de délai d’attente pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> ou pour le `initialSilenceTimeout` paramètre.  
  
-   Le moteur de reconnaissance vocale de détecte, mais ne trouve aucune correspondance dans un de ses chargés et activés <xref:System.Speech.Recognition.Grammar> objets.  
  
 Pour effectuer la reconnaissance asynchrone, utilisez une de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> méthodes.  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale élémentaire. L’exemple crée un <xref:System.Speech.Recognition.DictationGrammar>, il charge dans un module de reconnaissance vocale d’intra-processus et effectue une opération de reconnaissance.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Démarre une opération de reconnaissance vocale asynchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Ces méthodes effectuent unique ou plusieurs opérations de reconnaissance asynchrone. Le module de reconnaissance effectue chaque opération sur sa syntaxe de reconnaissance vocale chargés et activés.  
  
 Pendant un appel à cette méthode, le module de reconnaissance peut déclencher des événements suivants :  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Déclenché lorsque le module de reconnaissance détecte l’entrée qu’il peut identifier comme vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Déclenché lorsque l’entrée crée une correspondance ambiguë avec l’une des grammaires actives.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Déclenché lorsque le module de reconnaissance finalise un module de reconnaissance.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Déclenché lorsqu’un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> fin de l’opération.  
  
 Pour récupérer le résultat d’une opération de reconnaissance asynchrone, attacher un gestionnaire d’événements pour le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événement. Le module de reconnaissance déclenche cet événement chaque fois qu’il a terminé une opération de reconnaissance synchrones ou asynchrones. Si la reconnaissance n’a pas réussie, le <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> propriété sur <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> objet auquel vous pouvez accéder dans le gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> , cette manifestation se déroulera `null`.  
  
 Une opération de reconnaissance asynchrone peut échouer pour les raisons suivantes :  
  
-   Reconnaissance vocale n’est pas détecté avant l’expirent des intervalles de délai d’attente pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriétés.  
  
-   Le moteur de reconnaissance vocale de détecte, mais ne trouve aucune correspondance dans un de ses chargés et activés <xref:System.Speech.Recognition.Grammar> objets.  
  
-   Le <xref:System.Speech.Recognition.SpeechRecognitionEngine> doit avoir au moins un <xref:System.Speech.Recognition.Grammar> objet chargé avant d’effectuer la reconnaissance. Pour charger une grammaire de reconnaissance vocale, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> (méthode).  
  
-   Pour modifier la façon dont le module de reconnaissance gère le minutage de reconnaissance vocale ou silence en ce qui concerne la reconnaissance, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriétés.  
  
-   Pour effectuer la reconnaissance synchrone, utilisez une de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> méthodes.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Effectue une opération de reconnaissance vocale unique et asynchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cette méthode effectue une opération de reconnaissance unique et asynchrone. Le module de reconnaissance effectue l’opération par rapport à sa syntaxe de reconnaissance vocale chargés et activés.  
  
 Pendant un appel à cette méthode, le module de reconnaissance peut déclencher des événements suivants :  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Déclenché lorsque le module de reconnaissance détecte l’entrée qu’il peut identifier comme vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Déclenché lorsque l’entrée crée une correspondance ambiguë avec l’une des grammaires actives.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Déclenché lorsque le module de reconnaissance finalise un module de reconnaissance.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Déclenché lorsqu’un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> fin de l’opération.  
  
 Pour récupérer le résultat d’une opération de reconnaissance asynchrone, attacher un gestionnaire d’événements pour le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événement. Le module de reconnaissance déclenche cet événement chaque fois qu’il a terminé une opération de reconnaissance synchrones ou asynchrones. Si la reconnaissance n’a pas réussie, le <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> propriété sur <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> objet auquel vous pouvez accéder dans le gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> , cette manifestation se déroulera `null`.  
  
 Pour effectuer la reconnaissance synchrone, utilisez une de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> méthodes.  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale asynchrone élémentaire. L’exemple crée un <xref:System.Speech.Recognition.DictationGrammar>, il charge dans un module de reconnaissance vocale d’intra-processus et effectue une opération de reconnaissance asynchrone. Gestionnaires d’événements sont inclus pour illustrer les événements que le module de reconnaissance déclenche pendant l’opération.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">Indique s'il faut exécuter une ou de plusieurs opérations de reconnaissance.</param>
        <summary>Exécute une ou plusieurs opérations de reconnaissance vocale asynchrones.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si `mode` est <xref:System.Speech.Recognition.RecognizeMode.Multiple>, le module de reconnaissance continue d’effectuer des opérations de reconnaissance asynchrone jusqu'à ce que le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> méthode est appelée.  
  
 Pendant un appel à cette méthode, le module de reconnaissance peut déclencher des événements suivants :  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Déclenché lorsque le module de reconnaissance détecte l’entrée qu’il peut identifier comme vocale.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Déclenché lorsque l’entrée crée une correspondance ambiguë avec l’une des grammaires actives.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Déclenché lorsque le module de reconnaissance finalise un module de reconnaissance.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Déclenché lorsqu’un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> fin de l’opération.  
  
 Pour récupérer le résultat d’une opération de reconnaissance asynchrone, attacher un gestionnaire d’événements pour le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événement. Le module de reconnaissance déclenche cet événement chaque fois qu’il a terminé une opération de reconnaissance synchrones ou asynchrones. Si la reconnaissance n’a pas réussie, le <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> propriété sur <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> objet auquel vous pouvez accéder dans le gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> , cette manifestation se déroulera `null`.  
  
 Une opération de reconnaissance asynchrone peut échouer pour les raisons suivantes :  
  
-   Reconnaissance vocale n’est pas détecté avant l’expirent des intervalles de délai d’attente pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriétés.  
  
-   Le moteur de reconnaissance vocale de détecte, mais ne trouve aucune correspondance dans un de ses chargés et activés <xref:System.Speech.Recognition.Grammar> objets.  
  
 Pour effectuer la reconnaissance synchrone, utilisez une de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> méthodes.  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale asynchrone élémentaire. L’exemple crée un <xref:System.Speech.Recognition.DictationGrammar>, il charge dans un module de reconnaissance vocale d’intra-processus et effectue plusieurs opérations de reconnaissance asynchrone. Les opérations asynchrones sont annulées après 30 secondes. Gestionnaires d’événements sont inclus pour illustrer les événements que le module de reconnaissance déclenche pendant l’opération.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Met fin à la reconnaissance asynchrone sans attendre que l'opération de reconnaissance en cours se termine.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cette méthode finalise immédiatement le module de reconnaissance asynchrone. Si l’opération de reconnaissance asynchrone actuelle reçoit l’entrée, l’entrée est tronquée et l’opération se termine avec l’entrée existante. Le module de reconnaissance déclenche le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> événement lorsqu’une opération asynchrone est annulée et définit le <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> propriété de la <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> à `true`. Cette méthode annule les opérations asynchrones initiées par le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> méthodes.  
  
 Pour arrêter le module de reconnaissance asynchrone sans tronquer l’entrée, utilisez la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> (méthode).  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui illustre l’utilisation de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> (méthode). L’exemple crée et charge une syntaxe de reconnaissance vocale, lance une opération de reconnaissance asynchrone continue, puis s’interrompt 2 secondes avant d’annuler l’opération. Le module de reconnaissance reçoit l’entrée à partir du fichier, c:\temp\audioinput\sample.wav. Gestionnaires d’événements sont inclus pour illustrer les événements que le module de reconnaissance déclenche pendant l’opération.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Arrête la reconnaissance asynchrone lorsque l'opération de reconnaissance active se termine.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cette méthode finalise un module de reconnaissance asynchrone sans troncation d’entrée. Si l’opération de reconnaissance asynchrone actuelle reçoit l’entrée, le module de reconnaissance continue d’accepter une entrée avant la fin de l’opération de reconnaissance en cours. Le module de reconnaissance déclenche le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> événement lorsqu’une opération asynchrone est arrêtée et définit le <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> propriété de la <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> à `true`. Cette méthode arrête les opérations asynchrones initiées par le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> méthodes.  
  
 Pour annuler immédiatement le module de reconnaissance asynchrone avec uniquement l’entrée existante, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> (méthode).  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui illustre l’utilisation de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> (méthode). L’exemple crée et charge une syntaxe de reconnaissance vocale, lance une opération de reconnaissance asynchrone continue, puis s’interrompt 2 secondes avant d’arrêter l’opération. Le module de reconnaissance reçoit l’entrée à partir du fichier, c:\temp\audioinput\sample.wav. Gestionnaires d’événements sont inclus pour illustrer les événements que le module de reconnaissance déclenche pendant l’opération.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Déclenché lorsque <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> finalise un module de reconnaissance asynchrone.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le <xref:System.Speech.Recognition.SpeechRecognitionEngine> l’objet <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> méthode lance une opération de reconnaissance asynchrone. Lorsque le module de reconnaissance finalise l’opération asynchrone, il déclenche cet événement.  
  
 À l’aide du gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> événement, vous pouvez accéder à la <xref:System.Speech.Recognition.RecognitionResult> dans le <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> objet. Si la reconnaissance n’a pas réussie, <xref:System.Speech.Recognition.RecognitionResult> sera `null`. Pour déterminer si un délai d’attente ou une interruption de l’entrée audio a provoqué la reconnaissance à échouer, vous pouvez accéder aux propriétés pour <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, ou <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.  
  
 Pour plus d'informations, consultez la classe <xref:System.Speech.Recognition.RecognizeCompletedEventArgs>.  
  
 Pour obtenir plus d’informations sur les meilleurs candidats de reconnaissance rejetés, attacher un gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> événement.  
  
 Lorsque vous créez un délégué <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>, vous identifiez la méthode qui gérera l'événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant reconnaît des expressions telles que « Affichent la liste des artistes dans la catégorie jazz » ou « Affichent les êtes albums ». L’exemple utilise un gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> événement pour afficher des informations sur les résultats de la reconnaissance dans la console.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient la position actuelle du <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> dans l'entrée audio qu'il gère.</summary>
        <value>Position du module de reconnaissance dans l'entrée audio qu'il gère.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 La position audio est spécifique à chaque module de reconnaissance vocale. La valeur zéro d’un flux d’entrée est établie lorsqu’il est activé.  
  
 Le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> références de propriété le <xref:System.Speech.Recognition.SpeechRecognitionEngine> position de l’objet dans son entrée audio. En revanche, le <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propriété référence la position du périphérique d’entrée dans son flux audio généré. Ces positions peuvent être différentes. Par exemple, si le module de reconnaissance a reçu d’entrée pour lesquels il n’a pas encore généré un résultat de reconnaissance ensuite la valeur de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> propriété est inférieure à la valeur de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propriété.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtient des informations sur l'instance actuelle de <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Informations sur le module de reconnaissance vocale actuel.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Pour obtenir des informations sur tous les modules de reconnaissance vocale installés pour le système actuel, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> (méthode).  
  
   
  
## Examples  
 L’exemple suivant obtient une liste partielle des données pour le moteur de reconnaissance vocale de dans le processus actuel. Pour plus d'informations, consultez <xref:System.Speech.Recognition.RecognizerInfo>.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Déclenché lorsque <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> en cours d'exécution est suspendu pour accepter les modifications.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Les applications doivent utiliser <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> pour suspendre une instance en cours d’exécution de <xref:System.Speech.Recognition.SpeechRecognitionEngine> avant de modifier ses paramètres ou son <xref:System.Speech.Recognition.Grammar> objets. Le <xref:System.Speech.Recognition.SpeechRecognitionEngine> déclenche cet événement lorsqu’il est prêt à accepter les modifications.  
  
 Par exemple, bien que le <xref:System.Speech.Recognition.SpeechRecognitionEngine> est suspendu, vous pouvez charger, décharger, activer et désactiver <xref:System.Speech.Recognition.Grammar> objets et modifier des valeurs pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> propriétés. Pour plus d'informations, voir la méthode <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>.  
  
 Lorsque vous créez un délégué <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>, vous identifiez la méthode qui gérera l'événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant montre une application console qui charge et décharge <xref:System.Speech.Recognition.Grammar> objets. L’application utilise le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> méthode pour demander le moteur de reconnaissance vocale en pause afin qu’elle peut recevoir une mise à jour. L’application, puis charge ou décharge un <xref:System.Speech.Recognition.Grammar> objet.  
  
 À chaque mise à jour, un gestionnaire pour <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> d’événement écrit le nom et l’état d’actuellement chargés <xref:System.Speech.Recognition.Grammar> objets dans la console. Comme les grammaires sont chargées et déchargées, l’application reconnaît tout d’abord les noms des animaux de batterie de serveurs, puis les noms des animaux de batterie de serveurs et les noms des fruits, puis uniquement les noms de fruits.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Demande que le module de reconnaissance soit suspendu pour mettre à jour son état.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Utilisez cette méthode pour synchroniser les modifications apportées au module de reconnaissance. Par exemple, si vous chargez ou déchargez une grammaire de reconnaissance vocale pendant que le module de reconnaissance traite l’entrée, utilisez cette méthode et la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> événement pour synchroniser le comportement de votre application avec l’état du module de reconnaissance.  
  
 Lorsque cette méthode est appelée, le module de reconnaissance suspend ou termine les opérations asynchrones et génère un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> événement. Un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Gestionnaire d’événements pouvez ensuite modifier l’état du module de reconnaissance entre les opérations de reconnaissance. Lors de la gestion <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> événements, le module de reconnaissance s’interrompt jusqu'à ce que le Gestionnaire d’événements retourne.  
  
> [!NOTE]
>  Si l’entrée dans le module de reconnaissance est modifiée avant le module de reconnaissance déclenche le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> événement, la demande est ignorée.  
  
 Lorsque cette méthode est appelée :  
  
-   Si le module de reconnaissance ne traite pas d’entrée, le module de reconnaissance génère immédiatement la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> événement.  
  
-   Si le module de reconnaissance est le traitement d’entrée qui se compose de silence ou de bruit de fond, le module de reconnaissance s’interrompt l’opération de reconnaissance et génère le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> événement.  
  
-   Si le module de reconnaissance est le traitement d’entrée qui n’est pas constitué de silence ou de bruit de fond, le module de reconnaissance termine l’opération de reconnaissance, puis génère le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> événement.  
  
 Bien que le module de reconnaissance gère la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> événement :  
  
-   Le module de reconnaissance ne traite pas d’entrée et la valeur de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> propriété reste le même.  
  
-   Le module de reconnaissance continue à collecter d’entrée et la valeur de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propriété peut être modifiée.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Demande que le module de reconnaissance soit suspendu pour mettre à jour son état.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Lorsque le module de reconnaissance génère le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> événement, le <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> propriété de la <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> est `null`.  
  
 Pour fournir un jeton d’utilisateur, utilisez la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> (méthode). Pour spécifier un décalage de position audio, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> (méthode).  
  
   
  
## Examples  
 L’exemple suivant montre une application console qui charge et décharge <xref:System.Speech.Recognition.Grammar> objets. L’application utilise le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> méthode pour demander le moteur de reconnaissance vocale en pause afin qu’elle peut recevoir une mise à jour. L’application, puis charge ou décharge un <xref:System.Speech.Recognition.Grammar> objet.  
  
 À chaque mise à jour, un gestionnaire pour <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> d’événement écrit le nom et l’état d’actuellement chargés <xref:System.Speech.Recognition.Grammar> objets dans la console. Comme les grammaires sont chargées et déchargées, l’application reconnaît tout d’abord les noms des animaux de batterie de serveurs, puis les noms des animaux de batterie de serveurs et les noms des fruits, puis uniquement les noms de fruits.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Informations définies par l'utilisateur qui comporte des informations sur l'opération.</param>
        <summary>Demande que le module de reconnaissance soit suspendu pour mettre à jour son état et fournisse un jeton utilisateur pour l'événement associé.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Lorsque le module de reconnaissance génère le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> événement, le <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> propriété de la <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contient la valeur de la `userToken` paramètre.  
  
 Pour spécifier un décalage de position audio, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> (méthode).  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Informations définies par l'utilisateur qui comporte des informations sur l'opération.</param>
        <param name="audioPositionAheadToRaiseUpdate">Offset de la <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" /> actuelle pour différer la demande.</param>
        <summary>Demande que le module de reconnaissance soit suspendu pour mettre à jour son état et fournisse un offset et un jeton utilisateur pour l'événement associé.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance n’initie pas la demande de mise à jour du module de reconnaissance jusqu'à ce que le module de reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> est égal à actuel <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> plus `audioPositionAheadToRaiseUpdate`.  
  
 Lorsque le module de reconnaissance génère le <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> événement, le <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> propriété de la <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contient la valeur de la `userToken` paramètre.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">Flux d'entrée audio.</param>
        <param name="audioFormat">Format de l'entrée audio.</param>
        <summary>Configure l'objet <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> pour recevoir l'entrée d'un flux audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si le module de reconnaissance a atteint la fin du flux d’entrée pendant une opération de reconnaissance, l’opération de reconnaissance finalise avec l’entrée disponible. Toutes les opérations ultérieures de reconnaissance peuvent générer une exception, sauf si vous mettez à jour l’entrée dans le module de reconnaissance.  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale élémentaire. L’exemple utilise l’entrée à partir d’un fichier audio, example.wav, qui contient les expressions, « test du test d’un deux trois » et « laissez tomber cooper », séparés par une pause. L’exemple génère la sortie suivante.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Configure l'objet <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> pour recevoir l'entrée du périphérique audio par défaut.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre une reconnaissance vocale élémentaire. L’exemple utilise la sortie à partir du périphérique audio par défaut, effectue plusieurs opérations de reconnaissance asynchrone et s’arrête quand un utilisateur dicte la phrase, « exit ».  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Désactive l'entrée au module de reconnaissance vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Configurer le <xref:System.Speech.Recognition.SpeechRecognitionEngine> objet pour aucune entrée lorsque vous utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> méthodes, ou lorsque vous prenez un moteur de reconnaissance temporairement hors connexion.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">Chemin d'accès du fichier à utiliser en entrée.</param>
        <summary>Configure l'objet <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> pour recevoir l'entrée d'un fichier au format audio Waveform (.wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si le module de reconnaissance a atteint la fin du fichier d’entrée pendant une opération de reconnaissance, l’opération de reconnaissance finalise avec l’entrée disponible. Toutes les opérations ultérieures de reconnaissance peuvent générer une exception, sauf si vous mettez à jour l’entrée dans le module de reconnaissance.  
  
   
  
## Examples  
 L’exemple suivant effectue la reconnaissance de l’audio dans un fichier .wav et écrit le texte reconnu dans la console.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">Flux contenant les données audio.</param>
        <summary>Configure l'objet <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> pour recevoir l'entrée d'un flux qui contient des données au format audio Waveform (.wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si le module de reconnaissance a atteint la fin du flux d’entrée pendant une opération de reconnaissance, l’opération de reconnaissance finalise avec l’entrée disponible. Toutes les opérations ultérieures de reconnaissance peuvent générer une exception, sauf si vous mettez à jour l’entrée dans le module de reconnaissance.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Déclenché lorsque <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> détecte l'entrée qu'il peut identifier comme vocale.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Chaque module de reconnaissance vocale dispose d’un algorithme pour faire la distinction entre la latence et de reconnaissance vocale. Lorsque le <xref:System.Speech.Recognition.SpeechRecognitionEngine> effectue une opération de reconnaissance vocale, il déclenche le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> événement lors de son algorithme identifie l’entrée comme vocale. Le <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> propriété associé <xref:System.Speech.Recognition.SpeechDetectedEventArgs> objet indique l’emplacement dans le flux d’entrée où le module de reconnaissance détectée vocale. Le <xref:System.Speech.Recognition.SpeechRecognitionEngine> déclenche le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> événement avant de déclencher de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> événements.  
  
 Pour plus d’informations, consultez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> méthodes.  
  
 Lorsque vous créez un délégué <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, vous identifiez la méthode qui gérera l'événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant fait partie d’une application de console pour le choix des villes d’origine et de destination pour un vol. L’application reconnaît des expressions telles que « Je veux passage vers Miami à Chicago ».  L’exemple utilise le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> événement en rapport le <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> chaque heure de voix est détectée.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Déclenché lorsque <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> a identifié un mot ou des mots qui peuvent être un composant de plusieurs expressions complètes dans une syntaxe.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le <xref:System.Speech.Recognition.SpeechRecognitionEngine> génère de nombreux <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> événements tel qu’il tente d’identifier une phrase d’entrée. Vous pouvez accéder au texte d’expressions partiellement reconnus dans le <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propriété de la <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> objet dans le gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> événement. En règle générale, la gestion de ces événements est utile uniquement pour le débogage.  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> dérive de <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Pour plus d’informations, consultez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriété et la <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> méthodes.  
  
 Lorsque vous créez un délégué <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, vous identifiez la méthode qui gérera l'événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant reconnaît des expressions telles que « Afficher la liste des artistes dans la catégorie jazz ». L’exemple utilise le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> événement pour afficher des fragments de phrase incomplètes dans la console qu’elles sont reconnues.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Déclenché lorsque l'objet <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> reçoit l'entrée qui ne correspond pas à l'un de ses objets <see cref="T:System.Speech.Recognition.Grammar" /> chargés et activés.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Le module de reconnaissance déclenche cet événement s’il détermine qu’entrée ne correspond pas à une confiance suffisante des ses chargés et activés <xref:System.Speech.Recognition.Grammar> objets. Le <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propriété de la <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contient le rejet <xref:System.Speech.Recognition.RecognitionResult> objet. Vous pouvez utiliser le gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> événement afin de récupérer la reconnaissance <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> qui ont été rejetés et leur <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> scores.  
  
 Si votre application utilise un <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance, vous pouvez modifier le niveau de confiance à quels vocale entrée est acceptée ou rejetée avec l’un de le <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> méthodes. Vous pouvez modifier la façon dont la reconnaissance vocale répond aux non-reconnaissance vocale d’entrée à l’aide de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriétés.  
  
 Lorsque vous créez un délégué <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, vous identifiez la méthode qui gérera l'événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant reconnaît des expressions telles que « Affichent la liste des artistes dans la catégorie jazz » ou « Affichent les êtes albums ». L’exemple utilise un gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> événement pour afficher une notification dans la console lorsque la voix d’entrée ne peut pas être mis en correspondance avec le contenu de la grammaire avec suffisamment <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> pour produire un module de reconnaissance réussi. Le gestionnaire affiche également le résultat de la reconnaissance <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> qui ont été rejetés en raison des scores de confiance faible.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Déclenché lorsque l'objet <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> reçoit l'entrée qui correspond à l'un de ses objets <see cref="T:System.Speech.Recognition.Grammar" /> chargés et activés.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Vous pouvez lancer une opération de reconnaissance à l’aide de l’une de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> méthodes. Le module de reconnaissance déclenche le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événement s’il détermine qu’entrée correspond à l’un de ses chargé <xref:System.Speech.Recognition.Grammar> objets avec un niveau de confiance pour constituer la reconnaissance suffisant. Le <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propriété de la <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contient du acceptées <xref:System.Speech.Recognition.RecognitionResult> objet. Gestionnaires de <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événements peuvent obtenir l’expression reconnue, ainsi qu’une liste de la reconnaissance <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> avec des scores de confiance inférieur.  
  
 Si votre application utilise un <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance, vous pouvez modifier le niveau de confiance à quels vocale entrée est acceptée ou rejetée avec l’un de le <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> méthodes.  Vous pouvez modifier la façon dont la reconnaissance vocale répond aux non-reconnaissance vocale d’entrée à l’aide de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriétés.  
  
 Lorsque le module de reconnaissance reçoit l’entrée qui correspond à une grammaire, le <xref:System.Speech.Recognition.Grammar> objet peut déclencher son <xref:System.Speech.Recognition.Grammar.SpeechRecognized> événement. Le <xref:System.Speech.Recognition.Grammar> l’objet <xref:System.Speech.Recognition.Grammar.SpeechRecognized> événement est déclenché avant la reconnaissance vocale <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événement. Toutes les tâches spécifiques à une syntaxe spécifique doivent toujours être effectuées par un gestionnaire pour le <xref:System.Speech.Recognition.Grammar.SpeechRecognized> événement.  
  
 Lorsque vous créez un délégué <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, vous identifiez la méthode qui gérera l'événement. Pour associer l'événement au gestionnaire d'événements, ajoutez une instance du délégué à l'événement. Le gestionnaire d'événements est appelé chaque fois qu'un événement se produit, sauf si vous supprimez le délégué. Pour plus d’informations sur les délégués de gestionnaire d’événements, consultez [événements et délégués](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 L’exemple suivant fait partie d’une application console qui crée la grammaire de reconnaissance vocale, constructions un <xref:System.Speech.Recognition.Grammar> de l’objet et les charge dans le <xref:System.Speech.Recognition.SpeechRecognitionEngine> pour effectuer la reconnaissance. L’exemple montre la saisie vocale pour un <xref:System.Speech.Recognition.SpeechRecognitionEngine>, les résultats de reconnaissance associées et les événements associés déclenchés par le module de reconnaissance vocale.  
  
 Parlée entrée comme « Je veux passage de Chicago à Miami » déclenchera un <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événement. À propos de l’expression « Piloter me de Houston à Chicago » ne déclenche pas une <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événement.  
  
 L’exemple utilise un gestionnaire pour le <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> événement pour afficher correctement reconnu des expressions et la sémantique qu’ils contiennent dans la console.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Décharge tous les objets <see cref="T:System.Speech.Recognition.Grammar" /> du module de reconnaissance.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si le module de reconnaissance charge actuellement un <xref:System.Speech.Recognition.Grammar> attend de façon asynchrone, cette méthode jusqu'à ce que le <xref:System.Speech.Recognition.Grammar> est chargée, avant qu’il décharge toutes le <xref:System.Speech.Recognition.Grammar> objets à partir de la <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance.  
  
 Pour décharger une syntaxe spécifique, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> (méthode).  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre le chargement synchrone et le déchargement des grammaires de la reconnaissance vocale.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Objet de grammaire à décharger.</param>
        <summary>Décharge un objet <see cref="T:System.Speech.Recognition.Grammar" /> spécifié de l'instance <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si le module de reconnaissance est en cours d’exécution, les applications doivent utiliser <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> pour suspendre la <xref:System.Speech.Recognition.SpeechRecognitionEngine> instance avant le chargement, déchargement, activer ou désactiver un <xref:System.Speech.Recognition.Grammar> objet. À décharger tout <xref:System.Speech.Recognition.Grammar> objets, utiliser le <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> (méthode).  
  
   
  
## Examples  
 L’exemple suivant montre une partie d’une application console qui montre le chargement synchrone et le déchargement des grammaires de la reconnaissance vocale.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> a la valeur <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">La grammaire n'est pas chargée dans ce module de reconnaissance, ou ce module de reconnaissance charge actuellement la grammaire de façon asynchrone.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Met à jour la valeur d'un paramètre du module de reconnaissance.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Paramètres de module de reconnaissance peuvent contenir des données de chaîne, entier 64 bits ou mémoire adresse. Le tableau suivant décrit les paramètres qui sont définis pour une API Microsoft Speech (SAPI)-module de reconnaissance conforme. Les paramètres suivants doivent avoir la même plage pour chaque module de reconnaissance qui prend en charge le paramètre. Un module de reconnaissance vocale compatible SAPI n’est pas nécessaire pour prendre en charge ces paramètres et peut prendre en charge les autres paramètres.  
  
|Name|Description |  
|----------|-----------------|  
|`ResourceUsage`|Spécifie la consommation du processeur du module de reconnaissance. La plage est comprise entre 0 et 100. La valeur par défaut est 50.|  
|`ResponseSpeed`|Indique la longueur de silence à la fin de l’entrée sans ambiguïté avant que le module de reconnaissance vocale termine une opération de reconnaissance. La plage est comprise entre 0 et 10 000 millisecondes (ms). Ce paramètre correspond à la reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> propriété. Par défaut = 150ms.|  
|`ComplexResponseSpeed`|Indique la longueur de la latence en millisecondes (ms) à la fin de l’entrée ambiguë avant que le module de reconnaissance vocale termine une opération de reconnaissance. La plage est comprise entre 0 à 10 000 ms. Ce paramètre correspond à la reconnaissance <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriété. Par défaut = 500 ms.|  
|`AdaptationOn`|Indique si une adaptation du modèle acoustique est activé (valeur = `1`) ou OFF (valeur = `0`). La valeur par défaut est `1` (ON).|  
|`PersistedBackgroundAdaptation`|Indique si une adaptation en arrière-plan est activé (valeur = `1`) ou OFF (valeur = `0`), et conserve le paramètre dans le Registre. La valeur par défaut est `1` (ON).|  
  
 Pour retourner un des paramètres du module de reconnaissance, utilisez le <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> (méthode).  
  
 À l’exception de `PersistedBackgroundAdaptation`, les valeurs de propriété définies à l’aide de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> méthodes restent effectives uniquement pour l’instance actuelle de <xref:System.Speech.Recognition.SpeechRecognitionEngine>, après laquelle ils revenir à leurs paramètres par défaut.  
  
 Vous pouvez modifier la façon dont la reconnaissance vocale répond aux non-reconnaissance vocale d’entrée à l’aide de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, et <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriétés.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">Nom du paramètre à mettre à jour.</param>
        <param name="updatedValue">Nouvelle valeur du paramètre.</param>
        <summary>Met à jour le paramètre spécifié pour <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> avec la valeur entière spécifiée.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 À l’exception de `PersistedBackgroundAdaptation`, les valeurs de propriété définies à l’aide de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> méthode restent effectives uniquement pour l’instance actuelle de <xref:System.Speech.Recognition.SpeechRecognitionEngine>, après laquelle ils revenir à leurs paramètres par défaut. Consultez <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> pour obtenir une description des paramètres pris en charge.  
  
   
  
## Examples  
 L’exemple suivant fait partie d’une application console qui génère les valeurs pour un nombre de paramètres définis pour le module de reconnaissance qui prend en charge les paramètres régionaux en-US. L’exemple met à jour les paramètres de niveau de confiance et interroge ensuite le module de reconnaissance pour vérifier les valeurs mises à jour. L’exemple génère la sortie suivante.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> a la valeur <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> est la chaîne vide ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Le module de reconnaissance n'a pas de paramètre correspondant à ce nom.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Nom du paramètre à mettre à jour.</param>
        <param name="updatedValue">Nouvelle valeur du paramètre.</param>
        <summary>Met à jour le paramètre du moteur de reconnaissance vocale spécifié avec la valeur de chaîne spécifiée.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 À l’exception de `PersistedBackgroundAdaptation`, les valeurs de propriété définies à l’aide de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> méthode restent effectives uniquement pour l’instance actuelle de <xref:System.Speech.Recognition.SpeechRecognitionEngine>, après laquelle ils revenir à leurs paramètres par défaut. Consultez <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> pour obtenir une description des paramètres pris en charge.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> a la valeur <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> est la chaîne vide ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Le module de reconnaissance n'a pas de paramètre correspondant à ce nom.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>